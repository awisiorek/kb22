{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 3: Korpusannotation mit stanza\n",
    "\n",
    "- **stanza = Stanford Python NLP Tools**\n",
    "  - https://stanfordnlp.github.io/stanza/\n",
    "  - trainiert auf Universal-Dependencies-Korpora (**UD**):\n",
    "    - https://stanfordnlp.github.io/stanza/available_models.html\n",
    "  - verwendet ausschließlich neuronale Modelle\n",
    "\n",
    "- **Processors:**\n",
    "  - Tokenization\n",
    "  - Lemmatization\n",
    "  - POS Tagging\n",
    "  - Syntactic Parsing\n",
    "  - Named Entity Recognition\n",
    "  - Sentiment Analysis\n",
    "\n",
    "\n",
    "- https://www.nltk.org/book/ch05.html (Tagging)\n",
    "- https://www.nltk.org/book/ch06.html (Text Classification)\n",
    "- https://www.nltk.org/book/ch07.html (Information Extraction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part I: Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Automated Text Annotation\n",
    "\n",
    "#### = automatic analysis of linguistic data based on *NLP Classification Models* (Classifiers)\n",
    "\n",
    "- **Classification Task**: assign annotation class label to input\n",
    "  - **Tagging: word &rarr; annotation label**\n",
    "      - e.g. POS-label to each word in a sentence/text\n",
    "  - **Parsing: sentence &rarr; tree structure**\n",
    "\n",
    "- **Sequence Classification**: sentences/texts as sequences of linguistic units (words)\n",
    "\n",
    "\n",
    "- Annotation on different levels of linguistic description:\n",
    "  -  ** SEGMENTATION &rarr; MORPHOLOGICAL &rarr; SYNTACTIC &rarr; SEMANTIC ANALYSIS **\n",
    "- NLP Pipeline: models for different subsequent annotation tasks \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.nltk.org/images/tag-context.png\" width=44%>\n",
    "https://www.nltk.org/book/ch05.html#fig-tag-context\n",
    "> When we perform a language processing task based on unigrams, we are using one item of **context**. In the case of tagging, we only consider the current token, in isolation from any larger context. Given such a model, the best we can do is tag each word with its a priori most likely tag. This means we would tag a word such as *wind* with the same tag, regardless of whether it appears in the context *the wind* or *to wind*.\n",
    "\n",
    "> An n-gram tagger is a generalization of a unigram tagger whose context is the current word together with the part-of-speech tags of the n-1 preceding tokens, as shown in 5.1. The tag to be chosen, tn, is circled, and the context is shaded in grey. In the example of an n-gram tagger shown in 5.1, we have n=3; that is, we consider the tags of the two preceding words in addition to the current word. An n-gram tagger picks the tag that is most likely in the given context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Rule-based Models vs Machine Learning NLP Models\n",
    "\n",
    "### Rule-based Models\n",
    "- set of rules specifying a model \n",
    "- examples:\n",
    "  - rule-based tokenizer (see https://stanfordnlp.github.io/stanza/tokenize.html#use-spacy-for-fast-tokenization-and-sentence-segmentation)\n",
    "  - morphological regular expression tagger with NLTK: https://www.nltk.org/book/ch05.html#the-regular-expression-tagger\n",
    "\n",
    "\n",
    "### Machine Learning Models\n",
    "- learn rules (mapping from input to output) from training data\n",
    "- models based on patterns in sample data (training data)\n",
    "\n",
    "\n",
    "- different types of ML algorithms:\n",
    "    - statistical models \n",
    "    - neural network models (deep learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Training NLP Models: Feature Extraction + ML Algorithm\n",
    "\n",
    "- **learn mapping: *feature-representation of linguistic unit*  &rarr; *class label* **\n",
    "- **based on labeled training data**\n",
    "\n",
    "\n",
    "- training data for NLP models: annotated corpora (e.g. POS-annotated for POS tagger model)\n",
    "\n",
    "\n",
    "- result: Classification Model (Classifier) for predicting annotation labels (e.g. POS-tags)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.nltk.org/images/supervised-classification.png\" width=44%>\n",
    "https://www.nltk.org/book/ch06.html#fig-supervised-classification\n",
    ">Supervised Classification. <br>(a) During training, a feature extractor is used to convert each input value to a feature set [...] which capture the basic information about each input that should be used to classify it [...]. Pairs of feature sets and labels are fed into the machine learning algorithm to generate a model. <br>(b) During prediction, the same feature extractor is used to convert unseen inputs to feature sets. These feature sets are then fed into the model, which generates predicted labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## 5. Main Components of NLP Pipeline\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "> To start annotating text with Stanza, you would typically start by building a `Pipeline` that contains Processors, each fulfilling a specific NLP task you desire (e.g., tokenization, part-of-speech tagging, syntactic parsing, etc). The pipeline takes in raw text or a `Document` object that contains partial annotations, runs the specified processors in succession, and returns an annotated `Document` (see the documentation on `Document` for more information on how to extract these annotations).<br>https://stanfordnlp.github.io/stanza/pipeline.html\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "<img src=\"https://stanfordnlp.github.io/stanza/assets/images/pipeline.png\" width=44%>\n",
    "*Stanza Pipeline Overview, https://stanfordnlp.github.io/stanza/*\n",
    "\n",
    "\n",
    "---\n",
    "### Segmentation / Tokenization\n",
    "\n",
    "- split string into linguistic units (see https://www.nltk.org/book/ch03.html#sec-tokenization)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "### Lemmatization / Stemming\n",
    "\n",
    ">- stemming: strip off any affixes\n",
    "- lemmatization: resulting form as known word in a dictionary<br>https://www.nltk.org/book/ch03.html#sec-normalizing-text\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "###  POS tagging & morphological feature\n",
    "\n",
    "- assign Part-of-Speech-categories to words \n",
    "- example Tagsets: \n",
    "  - STTS for german texts: https://homepage.ruhr-uni-bochum.de/stephen.berman/Korpuslinguistik/Tagsets-STTS.html\n",
    "  - Universal Tagset: https://universaldependencies.org/u/pos/\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "###  Dependency parsing\n",
    "\n",
    "- identify syntactical relations between words (hierarchical sentence structure)\n",
    "- builds on POS-analysis\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Downstream Tasks\n",
    "\n",
    "\n",
    "### Named Entitiy Recognition (NER)\n",
    "\n",
    "- extract & classify named entities from text (persons, locations, organizations)\n",
    "- mapping:  **token/phrase &rarr; entity class**\n",
    "\n",
    "\n",
    "### Sentiment Analysis\n",
    "\n",
    "- classify text according to subjective attitude \n",
    "- mapping:  **text &rarr; sentiment label**\n",
    "\n",
    "\n",
    "\n",
    "### (Semantic Parsing, e.g. for Question-Answering-Systems)\n",
    "\n",
    "- obtain logical representations for sentences (sentence meaning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Example 1: Pipeline for Information Extraction System\n",
    "<img src=\"https://www.nltk.org/images/ie-architecture.png\" width=\"44%\">\n",
    "https://www.nltk.org/book/ch01.html#fig-sds:\n",
    "> Simple Pipeline Architecture for an Information Extraction System. This system takes the raw text of a document as its input, and generates a list of (entity, relation, entity) tuples as its output. For example, given a document that indicates that the company Georgia-Pacific is located in Atlanta, it might generate the tuple `([ORG: 'Georgia-Pacific'] 'in' [LOC: 'Atlanta'])`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Example 2: Pipeline for Spoken Dialogue System\n",
    "<img src=\"https://www.nltk.org/images/dialogue.png\" width=\"44%\">\n",
    "https://www.nltk.org/book/ch07.html#fig-ie-architecture:\n",
    "> Simple Pipeline Architecture for a Spoken Dialogue System: Spoken input (top left) is analyzed, words are recognized, sentences are parsed and interpreted in context, application-specific actions take place (top right); a response is planned, realized as a syntactic structure, then to suitably inflected words, and finally to spoken output; different types of linguistic knowledge inform each stage of the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part II: Using *stanza* for Text Annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ---\n",
    "  \n",
    "## stanza (Stanford Python NLP Tool)\n",
    "\n",
    "#### Python NLP package with models for many languages  (*NLP = Natural Language Processing*)\n",
    "\n",
    "> Stanza is a collection of accurate and efficient tools for the linguistic analysis of many human languages. Starting from raw text to syntactic analysis and entity recognition, Stanza brings state-of-the-art NLP models to languages of your choosing.<br>\n",
    " https://stanfordnlp.github.io/stanza/\n",
    " \n",
    "- provides linguistic annotation layers for raw text input \n",
    "- support for many languages (see https://stanfordnlp.github.io/stanza/available_models.html)\n",
    "- machine learning NLP models, built with neural network components\n",
    "- models trained on large corpora (based on [UD-formalism](https://universaldependencies.org/)) \n",
    " \n",
    "\n",
    "- native Python package, alternative to well-known **Stanford CoreNLP Tools** (java-based):\n",
    "    - Stanford CoreNLP Online Demo: https://corenlp.run/\n",
    "    - Stanford Parer Online: http://nlp.stanford.edu:8080/parser/\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Loading Package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##  0.1 Downloading models for different languages:\n",
    "\n",
    "https://stanfordnlp.github.io/stanza/installation_usage.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Ancient Languages Models\n",
    "\n",
    "- trained on PROIEL corpora\n",
    "- https://stanfordnlp.github.io/stanza/available_models.html\n",
    "\n",
    "\n",
    "\n",
    "####  Ancient Languages Corpora\n",
    "- Example: PROIEL Treebank: https://github.com/proiel \n",
    "  - \"A dependency treebank for ancient Indo-European languages\"\n",
    "  - Part of Syntacticus: http://dev.syntacticus.org/ (umbrella project for corpora of early IE languages)\n",
    "  \n",
    "  - Example: Latin PROIEL Corpus:\n",
    "    - Tabular conll-Format with dependency relations\n",
    "    - Download: https://raw.githubusercontent.com/proiel/proiel-treebank/master/latin-nt.conll\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "### Downloading Ancient Greek Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "197af6bcd4e8492dab2a49356d2765c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-29 10:42:33 INFO: Downloading default packages for language: grc (Ancient_Greek)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "302fe98b98854af28959fac022525a88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-29 10:43:00 INFO: Finished downloading models and saved to /stanza_resources.\n"
     ]
    }
   ],
   "source": [
    "#stanza.download('grc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Downloading Latin Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c2bdc7c80a44ca79666199e1b485720",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-29 10:43:25 INFO: Downloading default packages for language: la (Latin)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "045cb48adac6406babe0981afe524774",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-29 10:44:10 INFO: Finished downloading models and saved to /stanza_resources.\n"
     ]
    }
   ],
   "source": [
    "#stanza.download('la')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Downloading English & German Models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stanza.download('en')\n",
    "#stanza.download('de')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 0.2 Initialize `NLP`-Pipeline for Language (Load Models)\n",
    "\n",
    "https://stanfordnlp.github.io/stanza/pipeline.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-21 07:29:41 INFO: Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | combined  |\n",
      "| pos       | combined  |\n",
      "| lemma     | combined  |\n",
      "| depparse  | combined  |\n",
      "| sentiment | sstplus   |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "2022-01-21 07:29:41 INFO: Use device: cpu\n",
      "2022-01-21 07:29:41 INFO: Loading: tokenize\n",
      "2022-01-21 07:29:41 INFO: Loading: pos\n",
      "2022-01-21 07:29:41 INFO: Loading: lemma\n",
      "2022-01-21 07:29:41 INFO: Loading: depparse\n",
      "2022-01-21 07:29:42 INFO: Loading: sentiment\n",
      "2022-01-21 07:29:42 INFO: Loading: ner\n",
      "2022-01-21 07:29:43 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(lang='en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.3 Run Annotation on Sentence/Text\n",
    "\n",
    "### Creates `Document` Object\n",
    "\n",
    ">  Document object holds the annotation of an entire document, and is automatically generated when a string is annotated by the Pipeline. It contains a collection of Sentences and entities (which are represented as Spans), and can be seamlessly translated into a native Python object.<br>https://stanfordnlp.github.io/stanza/data_objects.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\n",
       "  [\n",
       "    {\n",
       "      \"id\": 1,\n",
       "      \"text\": \"To\",\n",
       "      \"lemma\": \"to\",\n",
       "      \"upos\": \"PART\",\n",
       "      \"xpos\": \"TO\",\n",
       "      \"head\": 2,\n",
       "      \"deprel\": \"mark\",\n",
       "      \"start_char\": 0,\n",
       "      \"end_char\": 2,\n",
       "      \"ner\": \"O\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": 2,\n",
       "      \"text\": \"be\",\n",
       "      \"lemma\": \"be\",\n",
       "      \"upos\": \"VERB\",\n",
       "      \"xpos\": \"VB\",\n",
       "      \"feats\": \"VerbForm=Inf\",\n",
       "      \"head\": 0,\n",
       "      \"deprel\": \"root\",\n",
       "      \"start_char\": 3,\n",
       "      \"end_char\": 5,\n",
       "      \"ner\": \"O\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": 3,\n",
       "      \"text\": \"or\",\n",
       "      \"lemma\": \"or\",\n",
       "      \"upos\": \"CCONJ\",\n",
       "      \"xpos\": \"CC\",\n",
       "      \"head\": 4,\n",
       "      \"deprel\": \"cc\",\n",
       "      \"start_char\": 6,\n",
       "      \"end_char\": 8,\n",
       "      \"ner\": \"O\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": 4,\n",
       "      \"text\": \"not\",\n",
       "      \"lemma\": \"not\",\n",
       "      \"upos\": \"PART\",\n",
       "      \"xpos\": \"RB\",\n",
       "      \"head\": 2,\n",
       "      \"deprel\": \"conj\",\n",
       "      \"start_char\": 9,\n",
       "      \"end_char\": 12,\n",
       "      \"ner\": \"O\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": 5,\n",
       "      \"text\": \"to\",\n",
       "      \"lemma\": \"to\",\n",
       "      \"upos\": \"PART\",\n",
       "      \"xpos\": \"TO\",\n",
       "      \"head\": 6,\n",
       "      \"deprel\": \"mark\",\n",
       "      \"start_char\": 13,\n",
       "      \"end_char\": 15,\n",
       "      \"ner\": \"O\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": 6,\n",
       "      \"text\": \"be\",\n",
       "      \"lemma\": \"be\",\n",
       "      \"upos\": \"VERB\",\n",
       "      \"xpos\": \"VB\",\n",
       "      \"feats\": \"VerbForm=Inf\",\n",
       "      \"head\": 2,\n",
       "      \"deprel\": \"xcomp\",\n",
       "      \"start_char\": 16,\n",
       "      \"end_char\": 18,\n",
       "      \"ner\": \"O\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": 7,\n",
       "      \"text\": \".\",\n",
       "      \"lemma\": \".\",\n",
       "      \"upos\": \"PUNCT\",\n",
       "      \"xpos\": \".\",\n",
       "      \"head\": 2,\n",
       "      \"deprel\": \"punct\",\n",
       "      \"start_char\": 18,\n",
       "      \"end_char\": 19,\n",
       "      \"ner\": \"O\"\n",
       "    }\n",
       "  ]\n",
       "]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp('To be or not to be.')\n",
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.4 Output Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: To \tlemma: to\tpos: PART\n",
      "word: be \tlemma: be\tpos: VERB\n",
      "word: or \tlemma: or\tpos: CCONJ\n",
      "word: not \tlemma: not\tpos: PART\n",
      "word: to \tlemma: to\tpos: PART\n",
      "word: be \tlemma: be\tpos: VERB\n",
      "word: . \tlemma: .\tpos: PUNCT\n"
     ]
    }
   ],
   "source": [
    "print(*[f'word: {word.text+\" \"}\\tlemma: {word.lemma}\\tpos: {word.pos}' for sent in doc.sentences for word in sent.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.5 Optional: Specifiy Processors (Annotation layers) = Building Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-21 07:30:28 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "| pos       | combined |\n",
      "| lemma     | combined |\n",
      "========================\n",
      "\n",
      "2022-01-21 07:30:28 INFO: Use device: cpu\n",
      "2022-01-21 07:30:28 INFO: Loading: tokenize\n",
      "2022-01-21 07:30:28 INFO: Loading: pos\n",
      "2022-01-21 07:30:28 INFO: Loading: lemma\n",
      "2022-01-21 07:30:28 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,pos,lemma') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention: Pipeline Requirements must be met!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-21 07:30:31 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| lemma     | combined |\n",
      "========================\n",
      "\n",
      "2022-01-21 07:30:31 INFO: Use device: cpu\n",
      "2022-01-21 07:30:31 INFO: Loading: lemma\n",
      "2022-01-21 07:30:31 INFO: \n",
      "\n"
     ]
    },
    {
     "ename": "PipelineRequirementsException",
     "evalue": "\n\n---\nPipeline Requirements Error!\n\tProcessor: LemmaProcessor\n\tPipeline processors list: lemma\n\tProcessor Requirements: {'tokenize'}\n\t\t- fulfilled: set()\n\t\t- missing: {'tokenize'}\n\nThe processors list provided for this pipeline is invalid.  Please make sure all prerequisites are met for every processor.\n\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPipelineRequirementsException\u001b[0m             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-d0a0335f19f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstanza\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'en'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'lemma'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Library/Python/3.6/lib/python/site-packages/stanza/pipeline/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, lang, dir, package, processors, logging_level, verbose, use_gpu, model_dir, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpipeline_reqs_exceptions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mPipelineRequirementsException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline_reqs_exceptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Done loading processors!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPipelineRequirementsException\u001b[0m: \n\n---\nPipeline Requirements Error!\n\tProcessor: LemmaProcessor\n\tPipeline processors list: lemma\n\tProcessor Requirements: {'tokenize'}\n\t\t- fulfilled: set()\n\t\t- missing: {'tokenize'}\n\nThe processors list provided for this pipeline is invalid.  Please make sure all prerequisites are met for every processor.\n\n\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(lang='en', processors='lemma') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## 1. Tokenization\n",
    "\n",
    "- Sentence & Word Tokenization \n",
    "- text as list of lists of words\n",
    "\n",
    "> Tokenization and sentence segmentation in Stanza are jointly performed by the TokenizeProcessor. This processor splits the raw input text into tokens and sentences, so that downstream annotation can happen at the sentence level.<br>https://stanfordnlp.github.io/stanza/tokenize.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-21 07:30:40 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "========================\n",
      "\n",
      "2022-01-21 07:30:40 INFO: Use device: cpu\n",
      "2022-01-21 07:30:40 INFO: Loading: tokenize\n",
      "2022-01-21 07:30:40 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: He \n",
      "word: does \n",
      "word: n't \n",
      "word: stop \n",
      "word: . \n"
     ]
    }
   ],
   "source": [
    "# initialize English neural pipeline\n",
    "nlp = stanza.Pipeline(lang='en', processors='tokenize')\n",
    "\n",
    "# run annotation:\n",
    "doc = nlp(\"\"\"He doesn't stop. \"\"\")\n",
    "\n",
    "# output:\n",
    "print(*[f'word: {word.text+\" \"}' for sent in doc.sentences for word in sent.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Multi-Word Token (MWT) Expansion (only for German, Spanish, French)\n",
    "- https://stanfordnlp.github.io/stanza/mwt.html\n",
    "- https://stanfordnlp.github.io/CoreNLP/mwt.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-21 07:30:40 INFO: Loading these models for language: de (German):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | gsd     |\n",
      "| mwt       | gsd     |\n",
      "=======================\n",
      "\n",
      "2022-01-21 07:30:40 INFO: Use device: cpu\n",
      "2022-01-21 07:30:40 INFO: Loading: tokenize\n",
      "2022-01-21 07:30:40 INFO: Loading: mwt\n",
      "2022-01-21 07:30:40 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: Ich \n",
      "word: sage \n",
      "word: es \n",
      "word: zu \n",
      "word: dem \n",
      "word: letzen \n",
      "word: Mal \n",
      "word: . \n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(lang='de', processors='tokenize, mwt')\n",
    "doc = nlp(\"\"\"Ich sage es zum letzen Mal.\"\"\")\n",
    "\n",
    "print(*[f'word: {word.text+\" \"}' for sent in doc.sentences for word in sent.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Tokenization without Sentence Segmentation\n",
    "\n",
    "> Sometimes you might want to tokenize your text given existing sentences (e.g., in machine translation). You can perform tokenization without sentence segmentation, as long as the sentences are split by two continuous newlines (\\n\\n) in the raw text.<br>https://stanfordnlp.github.io/stanza/tokenize.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-21 07:30:40 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "========================\n",
      "\n",
      "2022-01-21 07:30:40 INFO: Use device: cpu\n",
      "2022-01-21 07:30:40 INFO: Loading: tokenize\n",
      "2022-01-21 07:30:40 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Sentence 1 tokens =======\n",
      "id: (1,)\ttext: This\n",
      "id: (2,)\ttext: is\n",
      "id: (3,)\ttext: a\n",
      "id: (4,)\ttext: sentence\n",
      "id: (5,)\ttext: .\n",
      "====== Sentence 2 tokens =======\n",
      "id: (1,)\ttext: This\n",
      "id: (2,)\ttext: is\n",
      "id: (3,)\ttext: a\n",
      "id: (4,)\ttext: second\n",
      "id: (5,)\ttext: .\n",
      "id: (6,)\ttext: This\n",
      "id: (7,)\ttext: is\n",
      "id: (8,)\ttext: a\n",
      "id: (9,)\ttext: third\n",
      "id: (10,)\ttext: .\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(lang='en', processors='tokenize', tokenize_no_ssplit=True)\n",
    "doc = nlp('This is a sentence.\\n\\nThis is a second. This is a third.')\n",
    "\n",
    "for i, sentence in enumerate(doc.sentences):\n",
    "    print(f'====== Sentence {i+1} tokens =======')\n",
    "    print(*[f'id: {token.id}\\ttext: {token.text}' for token in sentence.tokens], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1.3 Start with Pretokenized Text\n",
    "\n",
    "> In some cases, you might have already tokenized your text, and just want to use Stanza for downstream processing. In these cases, you can feed in pretokenized (and sentence split) text to the pipeline, as newline (\\n) separated sentences, where each sentence is space separated tokens.<br>https://stanfordnlp.github.io/stanza/tokenize.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-21 07:30:40 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "========================\n",
      "\n",
      "2022-01-21 07:30:40 INFO: Use device: cpu\n",
      "2022-01-21 07:30:40 INFO: Loading: tokenize\n",
      "2022-01-21 07:30:40 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Sentence 1 tokens =======\n",
      "id: (1,)\ttext: This\n",
      "id: (2,)\ttext: is\n",
      "id: (3,)\ttext: token.ization\n",
      "id: (4,)\ttext: done\n",
      "id: (5,)\ttext: my\n",
      "id: (6,)\ttext: way!\n",
      "====== Sentence 2 tokens =======\n",
      "id: (1,)\ttext: Sentence\n",
      "id: (2,)\ttext: split,\n",
      "id: (3,)\ttext: too!\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(lang='en', processors='tokenize', tokenize_pretokenized=True)\n",
    "doc = nlp('This is token.ization done my way!\\nSentence split, too!')\n",
    "\n",
    "for i, sentence in enumerate(doc.sentences):\n",
    "    print(f'====== Sentence {i+1} tokens =======')\n",
    "    print(*[f'id: {token.id}\\ttext: {token.text}' for token in sentence.tokens], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Lemmatization\n",
    "\n",
    "https://stanfordnlp.github.io/stanza/lemma.html\n",
    "\n",
    "### *Requirements:* `tokenize, mwt, pos`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-21 07:30:40 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "| lemma     | combined |\n",
      "========================\n",
      "\n",
      "2022-01-21 07:30:40 INFO: Use device: cpu\n",
      "2022-01-21 07:30:40 INFO: Loading: tokenize\n",
      "2022-01-21 07:30:40 INFO: Loading: lemma\n",
      "2022-01-21 07:30:40 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: Call \tlemma: call\n",
      "word: me \tlemma: I\n",
      "word: Ishmael \tlemma: ishmael\n",
      "word: . \tlemma: .\n",
      "word: Some \tlemma: some\n",
      "word: years \tlemma: year\n",
      "word: ago \tlemma: ago\n",
      "word: — \tlemma: —\n",
      "word: never \tlemma: never\n",
      "word: mind \tlemma: mind\n",
      "word: how \tlemma: how\n",
      "word: long \tlemma: long\n",
      "word: precisely \tlemma: precisely\n",
      "word: — \tlemma: —\n",
      "word: having \tlemma: have\n",
      "word: little \tlemma: little\n",
      "word: or \tlemma: or\n",
      "word: no \tlemma: no\n",
      "word: money \tlemma: money\n",
      "word: in \tlemma: in\n",
      "word: my \tlemma: my\n",
      "word: purse \tlemma: purse\n",
      "word: , \tlemma: ,\n",
      "word: and \tlemma: and\n",
      "word: nothing \tlemma: nothing\n",
      "word: particular \tlemma: particular\n",
      "word: to \tlemma: to\n",
      "word: interest \tlemma: interest\n",
      "word: me \tlemma: I\n",
      "word: on \tlemma: on\n",
      "word: shore \tlemma: shore\n",
      "word: , \tlemma: ,\n",
      "word: I \tlemma: I\n",
      "word: thought \tlemma: think\n",
      "word: I \tlemma: I\n",
      "word: would \tlemma: would\n",
      "word: sail \tlemma: sail\n",
      "word: about \tlemma: about\n",
      "word: a \tlemma: a\n",
      "word: little \tlemma: little\n",
      "word: and \tlemma: and\n",
      "word: see \tlemma: see\n",
      "word: the \tlemma: the\n",
      "word: watery \tlemma: watery\n",
      "word: part \tlemma: part\n",
      "word: of \tlemma: of\n",
      "word: the \tlemma: the\n",
      "word: world \tlemma: world\n",
      "word: . \tlemma: .\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(lang='en', processors='tokenize, lemma') \n",
    "\n",
    "doc = nlp(\"\"\"Call me Ishmael. Some years ago—never mind how long precisely—having little or no money in my purse, and nothing particular to interest me on shore, I thought I would sail about a little and see the watery part of the world.\"\"\")\n",
    "\n",
    "print(*[f'word: {word.text+\" \"}\\tlemma: {word.lemma}' for sent in doc.sentences for word in sent.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##  3. Part-of-Speech & Morphological Features\n",
    "\n",
    "> The Part-of-Speech (POS) & morphological features tagging module labels words with their universal POS (UPOS) tags, treebank-specific POS (XPOS) tags, and universal morphological features (UFeats).<br>https://stanfordnlp.github.io/stanza/pos.html\n",
    "\n",
    "### *Requirements:* `tokenize, mwt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-21 07:30:40 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "| pos       | combined |\n",
      "========================\n",
      "\n",
      "2022-01-21 07:30:40 INFO: Use device: cpu\n",
      "2022-01-21 07:30:40 INFO: Loading: tokenize\n",
      "2022-01-21 07:30:40 INFO: Loading: pos\n",
      "2022-01-21 07:30:41 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: Call\tupos: VERB\txpos: VB\tfeats: Mood=Imp|VerbForm=Fin\n",
      "word: me\tupos: PRON\txpos: PRP\tfeats: Case=Acc|Number=Sing|Person=1|PronType=Prs\n",
      "word: Ishmael\tupos: PROPN\txpos: NNP\tfeats: Number=Sing\n",
      "word: .\tupos: PUNCT\txpos: .\tfeats: _\n",
      "word: Some\tupos: DET\txpos: DT\tfeats: _\n",
      "word: years\tupos: NOUN\txpos: NNS\tfeats: Number=Plur\n",
      "word: ago\tupos: ADV\txpos: RB\tfeats: Degree=Pos\n",
      "word: —\tupos: PUNCT\txpos: :\tfeats: _\n",
      "word: never\tupos: ADV\txpos: RB\tfeats: _\n",
      "word: mind\tupos: VERB\txpos: VB\tfeats: Mood=Imp|Person=2|VerbForm=Fin\n",
      "word: how\tupos: SCONJ\txpos: WRB\tfeats: PronType=Int\n",
      "word: long\tupos: ADV\txpos: RB\tfeats: Degree=Pos\n",
      "word: precisely\tupos: ADV\txpos: RB\tfeats: Degree=Pos\n",
      "word: —\tupos: PUNCT\txpos: :\tfeats: _\n",
      "word: having\tupos: VERB\txpos: VBG\tfeats: VerbForm=Ger\n",
      "word: little\tupos: ADJ\txpos: JJ\tfeats: Degree=Pos\n",
      "word: or\tupos: CCONJ\txpos: CC\tfeats: _\n",
      "word: no\tupos: DET\txpos: DT\tfeats: Polarity=Neg\n",
      "word: money\tupos: NOUN\txpos: NN\tfeats: Number=Sing\n",
      "word: in\tupos: ADP\txpos: IN\tfeats: _\n",
      "word: my\tupos: PRON\txpos: PRP$\tfeats: Number=Sing|Person=1|Poss=Yes|PronType=Prs\n",
      "word: purse\tupos: NOUN\txpos: NN\tfeats: Number=Sing\n",
      "word: ,\tupos: PUNCT\txpos: ,\tfeats: _\n",
      "word: and\tupos: CCONJ\txpos: CC\tfeats: _\n",
      "word: nothing\tupos: PRON\txpos: NN\tfeats: Number=Sing\n",
      "word: particular\tupos: ADJ\txpos: JJ\tfeats: Degree=Pos\n",
      "word: to\tupos: PART\txpos: TO\tfeats: _\n",
      "word: interest\tupos: VERB\txpos: VB\tfeats: VerbForm=Inf\n",
      "word: me\tupos: PRON\txpos: PRP\tfeats: Case=Acc|Number=Sing|Person=1|PronType=Prs\n",
      "word: on\tupos: ADP\txpos: IN\tfeats: _\n",
      "word: shore\tupos: NOUN\txpos: NN\tfeats: Number=Sing\n",
      "word: ,\tupos: PUNCT\txpos: ,\tfeats: _\n",
      "word: I\tupos: PRON\txpos: PRP\tfeats: Case=Nom|Number=Sing|Person=1|PronType=Prs\n",
      "word: thought\tupos: VERB\txpos: VBD\tfeats: Mood=Ind|Number=Sing|Person=1|Tense=Past|VerbForm=Fin\n",
      "word: I\tupos: PRON\txpos: PRP\tfeats: Case=Nom|Number=Sing|Person=1|PronType=Prs\n",
      "word: would\tupos: AUX\txpos: MD\tfeats: VerbForm=Fin\n",
      "word: sail\tupos: VERB\txpos: VB\tfeats: VerbForm=Inf\n",
      "word: about\tupos: ADP\txpos: IN\tfeats: _\n",
      "word: a\tupos: DET\txpos: DT\tfeats: Definite=Ind|PronType=Art\n",
      "word: little\tupos: ADJ\txpos: JJ\tfeats: Degree=Pos\n",
      "word: and\tupos: CCONJ\txpos: CC\tfeats: _\n",
      "word: see\tupos: VERB\txpos: VB\tfeats: VerbForm=Inf\n",
      "word: the\tupos: DET\txpos: DT\tfeats: Definite=Def|PronType=Art\n",
      "word: watery\tupos: ADJ\txpos: JJ\tfeats: Degree=Pos\n",
      "word: part\tupos: NOUN\txpos: NN\tfeats: Number=Sing\n",
      "word: of\tupos: ADP\txpos: IN\tfeats: _\n",
      "word: the\tupos: DET\txpos: DT\tfeats: Definite=Def|PronType=Art\n",
      "word: world\tupos: NOUN\txpos: NN\tfeats: Number=Sing\n",
      "word: .\tupos: PUNCT\txpos: .\tfeats: _\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,pos')\n",
    "doc = nlp(\"\"\"Call me Ishmael. Some years ago—never mind how long precisely—having little or no money in my purse, and nothing particular to interest me on shore, I thought I would sail about a little and see the watery part of the world.\"\"\")\n",
    "\n",
    "print(*[f'word: {word.text}\\tupos: {word.upos}\\txpos: {word.xpos}\\tfeats: {word.feats if word.feats else \"_\"}' for sent in doc.sentences for word in sent.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Dependency Parsing\n",
    "\n",
    "> The dependency parsing module builds a tree structure of words from the input sentence, which represents the syntactic dependency relations between words. The resulting tree representations, which follow the Universal Dependencies formalism, are useful in many downstream applications.<br>https://stanfordnlp.github.io/stanza/depparse.html\n",
    "\n",
    "\n",
    "### *Requirements:* `tokenize, mwt, pos, lemma`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-21 07:30:41 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "| pos       | combined |\n",
      "| lemma     | combined |\n",
      "| depparse  | combined |\n",
      "========================\n",
      "\n",
      "2022-01-21 07:30:41 INFO: Use device: cpu\n",
      "2022-01-21 07:30:41 INFO: Loading: tokenize\n",
      "2022-01-21 07:30:41 INFO: Loading: pos\n",
      "2022-01-21 07:30:41 INFO: Loading: lemma\n",
      "2022-01-21 07:30:41 INFO: Loading: depparse\n",
      "2022-01-21 07:30:42 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 1\tword: Chris\thead id: 3\thead: teaches\tdeprel: nsubj\n",
      "id: 2\tword: Manning\thead id: 1\thead: Chris\tdeprel: flat\n",
      "id: 3\tword: teaches\thead id: 0\thead: root\tdeprel: root\n",
      "id: 4\tword: at\thead id: 6\thead: University\tdeprel: case\n",
      "id: 5\tword: Stanford\thead id: 6\thead: University\tdeprel: compound\n",
      "id: 6\tword: University\thead id: 3\thead: teaches\tdeprel: obl\n",
      "id: 7\tword: .\thead id: 3\thead: teaches\tdeprel: punct\n",
      "id: 1\tword: He\thead id: 2\thead: lives\tdeprel: nsubj\n",
      "id: 2\tword: lives\thead id: 0\thead: root\tdeprel: root\n",
      "id: 3\tword: in\thead id: 6\thead: Area\tdeprel: case\n",
      "id: 4\tword: the\thead id: 6\thead: Area\tdeprel: det\n",
      "id: 5\tword: Bay\thead id: 6\thead: Area\tdeprel: compound\n",
      "id: 6\tword: Area\thead id: 2\thead: lives\tdeprel: obl\n",
      "id: 7\tword: .\thead id: 2\thead: lives\tdeprel: punct\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,pos,lemma,depparse')\n",
    "doc = nlp('Chris Manning teaches at Stanford University. He lives in the Bay Area.')\n",
    "\n",
    "print(*[f'id: {word.id}\\tword: {word.text}\\thead id: {word.head}\\thead: {sent.words[word.head-1].text if word.head > 0 else \"root\"}\\tdeprel: {word.deprel}' for sent in doc.sentences for word in sent.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### POS, Lemmatization & Dependencies with Ancient Greek & Latin Models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-21 07:30:42 INFO: Loading these models for language: grc (Ancient_Greek):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | proiel  |\n",
      "| pos       | proiel  |\n",
      "| lemma     | proiel  |\n",
      "| depparse  | proiel  |\n",
      "=======================\n",
      "\n",
      "2022-01-21 07:30:42 INFO: Use device: cpu\n",
      "2022-01-21 07:30:42 INFO: Loading: tokenize\n",
      "2022-01-21 07:30:42 INFO: Loading: pos\n",
      "2022-01-21 07:30:42 INFO: Loading: lemma\n",
      "2022-01-21 07:30:42 INFO: Loading: depparse\n",
      "2022-01-21 07:30:43 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: Ἄνδρα \tlemma: ἀνήρ\tpos: NOUN\tdeprel: obj\n",
      "word: μοι \tlemma: ἐγώ\tpos: PRON\tdeprel: iobj\n",
      "word: ἔννεπε, \tlemma: ἐννέπω\tpos: VERB\tdeprel: root\n",
      "word: Μοῦσα, \tlemma: Μοῦσα,\tpos: PROPN\tdeprel: obj\n",
      "word: πολύτροπον, \tlemma: πολύτροπος\tpos: ADJ\tdeprel: appos\n",
      "word: ὃς \tlemma: ὅς\tpos: PRON\tdeprel: nsubj:pass\n",
      "word: μάλα \tlemma: μάλα\tpos: ADV\tdeprel: advmod\n",
      "word: πολλὰ \tlemma: πολύς\tpos: ADJ\tdeprel: advmod\n",
      "word: πλάγχθη, \tlemma: πλάγω\tpos: VERB\tdeprel: acl\n",
      "word: ἐπεὶ \tlemma: ἐπεί\tpos: SCONJ\tdeprel: mark\n",
      "word: Τροίης \tlemma: Τροία\tpos: PROPN\tdeprel: nmod\n",
      "word: ἱερὸν \tlemma: ἱερόν\tpos: NOUN\tdeprel: obj\n",
      "word: πτολίεθρον \tlemma: πτολίεθρον\tpos: NOUN\tdeprel: obj\n",
      "word: ἔπερσε· \tlemma: περστίθημι\tpos: VERB\tdeprel: advcl\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(lang='grc')\n",
    "#text: https://de.wikipedia.org/wiki/Odyssee\n",
    "doc = nlp(\"\"\"Ἄνδρα μοι ἔννεπε, Μοῦσα, πολύτροπον, ὃς μάλα πολλὰ\n",
    "    πλάγχθη, ἐπεὶ Τροίης ἱερὸν πτολίεθρον ἔπερσε·\n",
    "    \"\"\")\n",
    "print(*[f'word: {word.text+\" \"}\\tlemma: {word.lemma}\\tpos: {word.pos}\\tdeprel: {word.deprel}' for sent in doc.sentences for word in sent.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-21 07:30:43 INFO: Loading these models for language: la (Latin):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | ittb    |\n",
      "| pos       | ittb    |\n",
      "| lemma     | ittb    |\n",
      "| depparse  | ittb    |\n",
      "=======================\n",
      "\n",
      "2022-01-21 07:30:43 INFO: Use device: cpu\n",
      "2022-01-21 07:30:43 INFO: Loading: tokenize\n",
      "2022-01-21 07:30:43 INFO: Loading: pos\n",
      "2022-01-21 07:30:43 INFO: Loading: lemma\n",
      "2022-01-21 07:30:44 INFO: Loading: depparse\n",
      "2022-01-21 07:30:44 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: Et \tlemma: Et\tpos: CCONJ\tdeprel: cc\n",
      "word: iam \tlemma: iam\tpos: ADV\tdeprel: advmod:emph\n",
      "word: iamque \tlemma: iamque\tpos: ADV\tdeprel: advmod\n",
      "word: magis \tlemma: magis\tpos: ADV\tdeprel: advmod\n",
      "word: cunctantem \tlemma: cunctans\tpos: NOUN\tdeprel: obj\n",
      "word: flectere \tlemma: flecto\tpos: VERB\tdeprel: xcomp\n",
      "word: sermo \tlemma: sermo\tpos: NOUN\tdeprel: nsubj\n",
      "word: coeperat \tlemma: coepi\tpos: VERB\tdeprel: root\n",
      "word: , \tlemma: ,\tpos: PUNCT\tdeprel: punct\n",
      "word: infelix \tlemma: infelix\tpos: NOUN\tdeprel: conj\n",
      "word: umero \tlemma: umerum\tpos: NOUN\tdeprel: orphan\n",
      "word: cum \tlemma: cum\tpos: SCONJ\tdeprel: mark\n",
      "word: apparuit \tlemma: appareo\tpos: VERB\tdeprel: advcl\n",
      "word: alto \tlemma: altus\tpos: NOUN\tdeprel: obl\n",
      "word: balteus \tlemma: balteus\tpos: ADJ\tdeprel: nsubj\n",
      "word: et \tlemma: et\tpos: CCONJ\tdeprel: cc\n",
      "word: notis \tlemma: nosco\tpos: VERB\tdeprel: conj\n",
      "word: fulserunt \tlemma: sulso\tpos: VERB\tdeprel: conj\n",
      "word: cingula \tlemma: cingula\tpos: NOUN\tdeprel: nsubj\n",
      "word: bullis \tlemma: bullum\tpos: NOUN\tdeprel: obl\n",
      "word: Pallantis \tlemma: Pallantis\tpos: VERB\tdeprel: acl\n",
      "word: pueri \tlemma: puer\tpos: NOUN\tdeprel: nmod\n",
      "word: , \tlemma: ,\tpos: PUNCT\tdeprel: punct\n",
      "word: victum \tlemma: victum\tpos: NOUN\tdeprel: obj\n",
      "word: quem \tlemma: qui\tpos: PRON\tdeprel: obj\n",
      "word: vulnere \tlemma: vulnere\tpos: NOUN\tdeprel: obl\n",
      "word: Turnus \tlemma: Turnus\tpos: PROPN\tdeprel: nsubj\n",
      "word: straverat \tlemma: straverat\tpos: VERB\tdeprel: acl:relcl\n",
      "word: atque \tlemma: atque\tpos: CCONJ\tdeprel: cc\n",
      "word: umeris \tlemma: umerus\tpos: NOUN\tdeprel: obl:arg\n",
      "word: inimicum \tlemma: inimicus\tpos: ADJ\tdeprel: amod\n",
      "word: insigne \tlemma: insignus\tpos: ADJ\tdeprel: obj\n",
      "word: gerebat \tlemma: gero\tpos: VERB\tdeprel: conj\n",
      "word: . \tlemma: .\tpos: PUNCT\tdeprel: punct\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(lang='la')\n",
    "#text: https://de.wikipedia.org/wiki/Aeneis#Textbeispiel:_Das_Ende_der_Aeneis_(12,_940%E2%80%93952)\n",
    "doc = nlp(\"\"\"Et iam iamque magis cunctantem flectere sermo\n",
    "    coeperat, infelix umero cum apparuit alto\n",
    "    balteus et notis fulserunt cingula bullis\n",
    "    Pallantis pueri, victum quem vulnere Turnus\n",
    "    straverat atque umeris inimicum insigne gerebat.\n",
    "    \"\"\")\n",
    "print(*[f'word: {word.text+\" \"}\\tlemma: {word.lemma}\\tpos: {word.pos}\\tdeprel: {word.deprel}' for sent in doc.sentences for word in sent.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##  5. Sentiment Analysis\n",
    "\n",
    "https://stanfordnlp.github.io/stanza/sentiment.html\n",
    "\n",
    "### *Requirements:* `tokenize`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-21 07:30:44 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "| sentiment | sstplus  |\n",
      "========================\n",
      "\n",
      "2022-01-21 07:30:44 INFO: Use device: cpu\n",
      "2022-01-21 07:30:44 INFO: Loading: tokenize\n",
      "2022-01-21 07:30:44 INFO: Loading: sentiment\n",
      "2022-01-21 07:30:45 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence 0 : sentiment: 1\n",
      "sentence 1 : sentiment: 0\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,sentiment')\n",
    "doc = nlp('I like. I hate.')\n",
    "\n",
    "for i, sentence in enumerate(doc.sentences):\n",
    "    print('sentence',i, ': sentiment:', sentence.sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Named Entity Recognition (NER)\n",
    "\n",
    "> The named entity recognition (NER) module recognizes mention spans of a particular entity type (e.g., Person or Organization) in the input sentence. NER is widely used in many NLP applications such as information extraction or question answering systems.<br>https://stanfordnlp.github.io/stanza/ner.html\n",
    "\n",
    "### *Requirements:* `tokenize, mwt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-21 07:30:45 INFO: Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | combined  |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "2022-01-21 07:30:45 INFO: Use device: cpu\n",
      "2022-01-21 07:30:45 INFO: Loading: tokenize\n",
      "2022-01-21 07:30:45 INFO: Loading: ner\n",
      "2022-01-21 07:30:46 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entity: Chris Manning\ttype: PERSON\n",
      "entity: Stanford University\ttype: ORG\n",
      "entity: the Bay Area\ttype: LOC\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,ner')\n",
    "doc = nlp(\"Chris Manning teaches at Stanford University. He lives in the Bay Area.\")\n",
    "\n",
    "# sentence based NER output\n",
    "print(*[f'entity: {ent.text}\\ttype: {ent.type}' for ent in doc.ents], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token: Chris\tner: B-PERSON\n",
      "token: Manning\tner: E-PERSON\n",
      "token: teaches\tner: O\n",
      "token: at\tner: O\n",
      "token: Stanford\tner: B-ORG\n",
      "token: University\tner: E-ORG\n",
      "token: .\tner: O\n",
      "token: He\tner: O\n",
      "token: lives\tner: O\n",
      "token: in\tner: O\n",
      "token: the\tner: B-LOC\n",
      "token: Bay\tner: I-LOC\n",
      "token: Area\tner: E-LOC\n",
      "token: .\tner: O\n"
     ]
    }
   ],
   "source": [
    "# token based NER output\n",
    "print(*[f'token: {token.text}\\tner: {token.ner}' for sent in doc.sentences for token in sent.tokens], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{\n",
      "  \"text\": \"Chris Manning\",\n",
      "  \"type\": \"PERSON\",\n",
      "  \"start_char\": 0,\n",
      "  \"end_char\": 13\n",
      "}, {\n",
      "  \"text\": \"Stanford University\",\n",
      "  \"type\": \"ORG\",\n",
      "  \"start_char\": 25,\n",
      "  \"end_char\": 44\n",
      "}, {\n",
      "  \"text\": \"the Bay Area\",\n",
      "  \"type\": \"LOC\",\n",
      "  \"start_char\": 58,\n",
      "  \"end_char\": 70\n",
      "}]\n"
     ]
    }
   ],
   "source": [
    "print(doc.entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Data Conversions\n",
    "\n",
    "https://stanfordnlp.github.io/stanza/data_conversion.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-21 07:30:47 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "| pos       | combined |\n",
      "| lemma     | combined |\n",
      "| depparse  | combined |\n",
      "========================\n",
      "\n",
      "2022-01-21 07:30:47 INFO: Use device: cpu\n",
      "2022-01-21 07:30:47 INFO: Loading: tokenize\n",
      "2022-01-21 07:30:47 INFO: Loading: pos\n",
      "2022-01-21 07:30:48 INFO: Loading: lemma\n",
      "2022-01-21 07:30:48 INFO: Loading: depparse\n",
      "2022-01-21 07:30:48 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,pos,lemma,depparse')\n",
    "doc = nlp('Chris Manning teaches at Stanford University. He lives in the Bay Area.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document to Python Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'deprel': 'nsubj',\n",
       "  'end_char': 5,\n",
       "  'feats': 'Number=Sing',\n",
       "  'head': 3,\n",
       "  'id': 1,\n",
       "  'lemma': 'Chris',\n",
       "  'start_char': 0,\n",
       "  'text': 'Chris',\n",
       "  'upos': 'PROPN',\n",
       "  'xpos': 'NNP'},\n",
       " {'deprel': 'flat',\n",
       "  'end_char': 13,\n",
       "  'feats': 'Number=Sing',\n",
       "  'head': 1,\n",
       "  'id': 2,\n",
       "  'lemma': 'Manning',\n",
       "  'start_char': 6,\n",
       "  'text': 'Manning',\n",
       "  'upos': 'PROPN',\n",
       "  'xpos': 'NNP'},\n",
       " {'deprel': 'root',\n",
       "  'end_char': 21,\n",
       "  'feats': 'Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin',\n",
       "  'head': 0,\n",
       "  'id': 3,\n",
       "  'lemma': 'teach',\n",
       "  'start_char': 14,\n",
       "  'text': 'teaches',\n",
       "  'upos': 'VERB',\n",
       "  'xpos': 'VBZ'},\n",
       " {'deprel': 'case',\n",
       "  'end_char': 24,\n",
       "  'head': 6,\n",
       "  'id': 4,\n",
       "  'lemma': 'at',\n",
       "  'start_char': 22,\n",
       "  'text': 'at',\n",
       "  'upos': 'ADP',\n",
       "  'xpos': 'IN'},\n",
       " {'deprel': 'compound',\n",
       "  'end_char': 33,\n",
       "  'feats': 'Number=Sing',\n",
       "  'head': 6,\n",
       "  'id': 5,\n",
       "  'lemma': 'Stanford',\n",
       "  'start_char': 25,\n",
       "  'text': 'Stanford',\n",
       "  'upos': 'PROPN',\n",
       "  'xpos': 'NNP'},\n",
       " {'deprel': 'obl',\n",
       "  'end_char': 44,\n",
       "  'feats': 'Number=Sing',\n",
       "  'head': 3,\n",
       "  'id': 6,\n",
       "  'lemma': 'University',\n",
       "  'start_char': 34,\n",
       "  'text': 'University',\n",
       "  'upos': 'PROPN',\n",
       "  'xpos': 'NNP'},\n",
       " {'deprel': 'punct',\n",
       "  'end_char': 45,\n",
       "  'head': 3,\n",
       "  'id': 7,\n",
       "  'lemma': '.',\n",
       "  'start_char': 44,\n",
       "  'text': '.',\n",
       "  'upos': 'PUNCT',\n",
       "  'xpos': '.'}]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dicts = doc.to_dict() # dicts is List[List[Dict]], representing each token / word in each sentence in the document\n",
    "dicts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Object to CoNLL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1',\n",
       "  'Chris',\n",
       "  'Chris',\n",
       "  'PROPN',\n",
       "  'NNP',\n",
       "  'Number=Sing',\n",
       "  '3',\n",
       "  'nsubj',\n",
       "  '_',\n",
       "  'start_char=0|end_char=5'],\n",
       " ['2',\n",
       "  'Manning',\n",
       "  'Manning',\n",
       "  'PROPN',\n",
       "  'NNP',\n",
       "  'Number=Sing',\n",
       "  '1',\n",
       "  'flat',\n",
       "  '_',\n",
       "  'start_char=6|end_char=13'],\n",
       " ['3',\n",
       "  'teaches',\n",
       "  'teach',\n",
       "  'VERB',\n",
       "  'VBZ',\n",
       "  'Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin',\n",
       "  '0',\n",
       "  'root',\n",
       "  '_',\n",
       "  'start_char=14|end_char=21'],\n",
       " ['4',\n",
       "  'at',\n",
       "  'at',\n",
       "  'ADP',\n",
       "  'IN',\n",
       "  '_',\n",
       "  '6',\n",
       "  'case',\n",
       "  '_',\n",
       "  'start_char=22|end_char=24'],\n",
       " ['5',\n",
       "  'Stanford',\n",
       "  'Stanford',\n",
       "  'PROPN',\n",
       "  'NNP',\n",
       "  'Number=Sing',\n",
       "  '6',\n",
       "  'compound',\n",
       "  '_',\n",
       "  'start_char=25|end_char=33'],\n",
       " ['6',\n",
       "  'University',\n",
       "  'University',\n",
       "  'PROPN',\n",
       "  'NNP',\n",
       "  'Number=Sing',\n",
       "  '3',\n",
       "  'obl',\n",
       "  '_',\n",
       "  'start_char=34|end_char=44'],\n",
       " ['7',\n",
       "  '.',\n",
       "  '.',\n",
       "  'PUNCT',\n",
       "  '.',\n",
       "  '_',\n",
       "  '3',\n",
       "  'punct',\n",
       "  '_',\n",
       "  'start_char=44|end_char=45']]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stanza.utils.conll import CoNLL\n",
    "conll = CoNLL.convert_dict(dicts) # conll is List[List[List]], representing each token / word in each sentence in the document\n",
    "conll[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Appendix: Neural Network Model for Tokenizer (stanza)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'config': {'anneal': 0.999,\n",
       "  'anneal_after': 2000,\n",
       "  'batch_size': 32,\n",
       "  'conll_file': '/tmp/tmpv66ud1ga',\n",
       "  'conv_filters': '1,9',\n",
       "  'conv_res': None,\n",
       "  'cpu': False,\n",
       "  'cuda': True,\n",
       "  'dev_conll_gold': '/home/john/stanza/data/tokenize/grc_proiel.dev.gold.conllu',\n",
       "  'dev_label_file': '/home/john/stanza/data/tokenize/grc_proiel-ud-dev.toklabels',\n",
       "  'dev_txt_file': '/home/john/stanza/data/tokenize/grc_proiel.dev.txt',\n",
       "  'dropout': 0.33,\n",
       "  'emb_dim': 32,\n",
       "  'epochs': 10,\n",
       "  'eval_steps': 200,\n",
       "  'feat_dim': 4,\n",
       "  'feat_funcs': ['space_before', 'capitalized', 'all_caps', 'numeric'],\n",
       "  'hidden_dim': 64,\n",
       "  'hier_invtemp': 0.5,\n",
       "  'hierarchical': True,\n",
       "  'input_dropout': False,\n",
       "  'label_file': '/home/john/stanza/data/tokenize/grc_proiel-ud-train.toklabels',\n",
       "  'lang': 'grc',\n",
       "  'load_name': None,\n",
       "  'lr0': 0.002,\n",
       "  'max_grad_norm': 1.0,\n",
       "  'max_seqlen': 300,\n",
       "  'max_steps_before_stop': 5000,\n",
       "  'mode': 'train',\n",
       "  'mwt_json_file': '/home/john/stanza/data/tokenize/grc_proiel-ud-dev-mwt.json',\n",
       "  'report_steps': 20,\n",
       "  'residual': True,\n",
       "  'rnn_layers': 1,\n",
       "  'save_dir': 'saved_models/tokenize',\n",
       "  'save_name': 'saved_models/tokenize/grc_proiel_tokenizer.pt',\n",
       "  'seed': 1234,\n",
       "  'sent_drop_prob': 0.2,\n",
       "  'shorthand': 'grc_proiel',\n",
       "  'shuffle_steps': 100,\n",
       "  'skip_newline': False,\n",
       "  'steps': 50000,\n",
       "  'tok_noise': 0.02,\n",
       "  'txt_file': '/home/john/stanza/data/tokenize/grc_proiel.train.txt',\n",
       "  'unit_dropout': 0.33,\n",
       "  'use_mwt': False,\n",
       "  'vocab_size': 178,\n",
       "  'weight_decay': 0.0},\n",
       " 'model': OrderedDict([('embeddings.weight',\n",
       "               tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "                       [-0.0312,  0.0456,  0.0169,  ..., -0.0369, -0.0564,  0.0440],\n",
       "                       [ 0.0692,  0.0348,  0.0372,  ..., -0.0245,  0.0169,  0.0056],\n",
       "                       ...,\n",
       "                       [-1.1057,  0.2742, -0.1665,  ..., -1.8364,  1.0788,  0.8288],\n",
       "                       [ 0.7613, -0.7287, -0.4129,  ...,  0.5182, -1.6537,  0.3007],\n",
       "                       [-0.2082,  1.5563, -0.0894,  ...,  0.4001, -0.9314,  1.5783]])),\n",
       "              ('rnn.weight_ih_l0',\n",
       "               tensor([[ 0.4099, -0.0461, -0.2977,  ...,  0.4540,  0.6260,  0.0903],\n",
       "                       [ 0.7176,  0.1043,  0.2129,  ..., -0.1269, -0.0463, -0.0302],\n",
       "                       [-0.0039, -0.1980,  0.1864,  ..., -0.5187, -0.5880,  0.0736],\n",
       "                       ...,\n",
       "                       [-0.6515, -0.0498, -0.3755,  ..., -0.0564,  0.0138,  0.0543],\n",
       "                       [ 0.1001, -0.3102,  0.7834,  ...,  1.7353,  1.7160, -0.0845],\n",
       "                       [ 0.7494,  0.5656,  0.2115,  ..., -0.8260, -0.6888,  0.0124]])),\n",
       "              ('rnn.weight_hh_l0',\n",
       "               tensor([[-0.9892, -0.0323,  0.9489,  ...,  0.4096,  0.8713,  0.3391],\n",
       "                       [-0.5388,  0.2720,  0.5072,  ..., -0.0126, -0.1073, -0.1432],\n",
       "                       [ 0.0298, -0.2379,  0.1672,  ..., -0.6577, -0.1999,  0.3213],\n",
       "                       ...,\n",
       "                       [-0.9633, -0.2906,  1.3894,  ..., -0.3216,  0.0202, -0.7302],\n",
       "                       [-0.1110, -0.6997,  0.3470,  ...,  0.4829, -0.5306, -1.1145],\n",
       "                       [-0.8052,  0.3709, -0.4785,  ...,  0.4396,  0.2607, -0.2138]])),\n",
       "              ('rnn.bias_ih_l0',\n",
       "               tensor([-3.7397e-01, -2.5325e-01,  2.6615e-01, -3.6893e-01, -2.5018e-01,\n",
       "                       -5.7185e-01, -6.1104e-01, -2.2050e-01, -4.9934e-01, -2.6615e-02,\n",
       "                        1.6194e-01, -5.7609e-01, -7.2981e-03, -5.4968e-01, -3.2486e-01,\n",
       "                       -5.8932e-01, -1.1040e+00, -1.0832e-01, -1.1906e-02, -1.1762e-01,\n",
       "                        3.8625e-02, -6.4501e-01, -4.6387e-01, -1.0210e+00, -3.4674e-01,\n",
       "                       -3.6812e-01,  3.7280e-01, -2.0852e-01,  2.5924e-02, -5.8193e-01,\n",
       "                        5.1940e-01, -2.3596e-01,  2.6734e-02, -5.7426e-01, -7.7671e-02,\n",
       "                        1.2629e-01, -3.5531e-01, -3.8357e-01, -1.9165e-01,  5.9918e-02,\n",
       "                       -9.4764e-02, -1.7737e-01, -2.7365e-01, -2.8448e-01, -8.3358e-02,\n",
       "                       -2.3042e-01,  3.3862e-01, -4.6629e-01, -2.3514e-01, -4.0042e-01,\n",
       "                       -3.9356e-01, -3.3020e-01, -9.7201e-03, -1.0738e-01, -1.4788e-01,\n",
       "                       -1.9410e-01, -7.1894e-02,  1.4811e-01, -1.8523e-01, -4.6320e-01,\n",
       "                        9.0235e-04, -2.9764e-01, -5.2322e-01, -5.2960e-01, -1.3074e-01,\n",
       "                        1.5233e-01, -5.4355e-01, -4.1054e-01, -4.7697e-01, -2.9716e-01,\n",
       "                       -6.8448e-01, -1.3742e-01,  3.1639e-02, -6.5065e-01, -2.0217e-01,\n",
       "                        1.9474e-02, -1.0518e-01, -9.4516e-02, -3.4288e-01, -3.0762e-01,\n",
       "                        7.9026e-01, -1.7823e-01, -4.5409e-01, -1.5523e-01, -2.6721e-01,\n",
       "                       -1.5923e-01, -3.2100e-01,  1.2508e+00, -4.5051e-01, -2.4155e-01,\n",
       "                       -5.0784e-01, -1.0471e-01, -4.9772e-01, -3.0201e-01, -1.8539e-01,\n",
       "                       -5.9021e-02, -7.5564e-02,  6.0978e-02, -1.3530e-01,  1.9087e-01,\n",
       "                       -6.0398e-01, -1.8683e-01, -3.2862e-01,  1.2821e-01, -3.2081e-02,\n",
       "                       -5.2044e-02,  1.8057e-01, -2.6551e-01, -6.2366e-01, -2.1233e-01,\n",
       "                       -6.0346e-01, -5.4585e-01,  2.9487e-01, -2.1781e-01, -1.3729e-01,\n",
       "                       -1.5305e-01, -4.6197e-01, -5.0605e-01, -5.6364e-01,  3.4390e-01,\n",
       "                        1.0833e-02, -3.1982e-01, -2.6465e-01, -1.4046e-01, -3.0802e-01,\n",
       "                        3.8826e-02,  1.8080e-02, -3.4828e-01,  1.4791e-01, -4.0302e-01,\n",
       "                       -3.5123e-01, -5.4173e-02, -5.3496e-01, -7.1658e-02,  1.7506e-01,\n",
       "                       -9.8935e-02,  1.8397e-01,  2.4655e-01,  1.4246e-01,  1.2542e-01,\n",
       "                        1.4206e-01, -3.2339e-02,  9.0468e-02,  2.2553e-01,  1.5934e-01,\n",
       "                       -3.1032e-02,  1.8794e-01,  7.1422e-02,  1.2905e-01, -3.1813e-01,\n",
       "                        5.4292e-02,  2.4559e-01, -3.5891e-02, -2.8039e-01, -4.8410e-04,\n",
       "                        5.4512e-02, -2.2648e-01, -1.6353e-01, -1.1980e-01, -1.3366e-01,\n",
       "                        1.3382e-01, -1.1919e-01, -1.4794e-01,  3.9179e-01, -7.0607e-02,\n",
       "                       -5.8753e-02, -2.6483e-02, -9.9385e-02,  3.3053e-01, -3.1390e-01,\n",
       "                        5.3637e-02, -3.1610e-01,  1.5904e-01, -1.1816e-01,  3.0087e-01,\n",
       "                        2.1261e-01, -2.0829e-01, -5.7340e-02, -4.3115e-01,  3.3152e-01,\n",
       "                       -3.6572e-01,  6.5834e-02,  2.3856e-01, -1.9176e-02,  7.1828e-03,\n",
       "                        5.6323e-04,  3.1567e-01,  6.9543e-02,  1.8231e-01,  2.4997e-01,\n",
       "                       -1.0727e-01, -8.1691e-02,  4.4045e-02, -4.7838e-01,  1.4325e-01,\n",
       "                       -2.7740e-01, -2.4476e-01, -2.1269e-01, -1.5286e-01,  2.3659e-01,\n",
       "                       -1.9712e-02,  3.2194e-01,  1.2890e-01,  1.5964e-01, -3.8566e-02,\n",
       "                       -1.0114e-01,  5.1895e-01,  1.2266e-01, -1.7860e-01, -1.9907e-01,\n",
       "                       -4.7039e-02, -2.3678e-01,  1.1994e-01, -1.4744e-01,  1.1220e-03,\n",
       "                        2.3002e-01, -6.1079e-03,  2.8217e-01,  1.8270e-01, -3.1585e-01,\n",
       "                        2.5321e-02, -8.7387e-02,  2.2377e-01, -1.6808e-01,  1.1612e-01,\n",
       "                        2.2706e-02,  5.9499e-02,  3.6087e-01,  2.3408e-02,  4.1587e-01,\n",
       "                        5.6627e-02, -2.4192e-02, -4.5007e-02, -6.7354e-03, -1.0027e-01,\n",
       "                        3.0139e-02,  1.2619e-02,  5.4391e-02,  2.5045e-01, -3.6042e-01,\n",
       "                       -1.4886e-02,  5.2335e-02,  3.0322e-02, -3.2929e-01, -1.3680e-01,\n",
       "                        8.3872e-02, -6.5809e-02,  2.5097e-02, -2.9974e-01, -2.5962e-01,\n",
       "                       -3.1480e-01,  9.7615e-02,  3.7875e-01,  1.1316e-01, -9.2757e-02,\n",
       "                       -3.9803e-02])),\n",
       "              ('rnn.bias_hh_l0',\n",
       "               tensor([-0.5163, -0.3539,  0.4133, -0.2648, -0.2278, -0.5405, -0.6263, -0.2602,\n",
       "                       -0.6729, -0.1229,  0.1807, -0.6176,  0.1313, -0.5361, -0.3896, -0.4579,\n",
       "                       -1.1700,  0.0927, -0.1198,  0.0979,  0.0432, -0.8292, -0.5632, -1.1359,\n",
       "                       -0.3810, -0.5605,  0.4754, -0.0297,  0.0366, -0.6761,  0.4045, -0.3529,\n",
       "                       -0.1441, -0.6143, -0.0617,  0.1198, -0.3284, -0.2878, -0.1165, -0.0953,\n",
       "                       -0.2565, -0.1115, -0.3002, -0.2836, -0.0986, -0.1063,  0.3478, -0.3624,\n",
       "                       -0.3107, -0.4294, -0.2782, -0.2321, -0.0454, -0.1021, -0.0147, -0.0401,\n",
       "                       -0.1985,  0.2308, -0.3017, -0.3749,  0.0664, -0.2729, -0.6177, -0.3211,\n",
       "                       -0.1450,  0.1292, -0.3044, -0.2870, -0.4789, -0.3493, -0.7967,  0.0099,\n",
       "                       -0.1020, -0.5927, -0.0659, -0.0652, -0.1255, -0.2467, -0.1210, -0.3224,\n",
       "                        0.8308, -0.2252, -0.4186,  0.0462, -0.4520, -0.1410, -0.4873,  1.1741,\n",
       "                       -0.4724, -0.2360, -0.5823, -0.1455, -0.5025, -0.1208, -0.2614, -0.1433,\n",
       "                       -0.1402, -0.0258, -0.2459,  0.2314, -0.5481, -0.3701, -0.2775,  0.1683,\n",
       "                        0.0216, -0.0349,  0.1698, -0.1242, -0.6047, -0.2033, -0.5099, -0.6233,\n",
       "                        0.1852, -0.1539, -0.0105,  0.0088, -0.4331, -0.5599, -0.6124,  0.4851,\n",
       "                        0.0074, -0.1076, -0.4512, -0.3209, -0.2735, -0.0302,  0.0183, -0.2618,\n",
       "                        0.1252, -0.3488, -0.3074, -0.1237, -0.6090, -0.1956,  0.0219, -0.1019,\n",
       "                        0.2372,  0.0833,  0.1691,  0.1636, -0.0149,  0.1884,  0.1909,  0.0904,\n",
       "                        0.2861, -0.0367, -0.0319,  0.1443, -0.0159, -0.1776,  0.0866,  0.0437,\n",
       "                       -0.0838, -0.2608,  0.0132,  0.0815, -0.2036, -0.2376, -0.2107, -0.0101,\n",
       "                        0.1636, -0.1326, -0.3143,  0.3307, -0.0740, -0.2482, -0.0821,  0.0100,\n",
       "                        0.3806, -0.2030,  0.0416, -0.1492,  0.1374, -0.0305,  0.2037,  0.1527,\n",
       "                       -0.1753, -0.2782, -0.3168,  0.1183, -0.2807,  0.2933,  0.1157,  0.0579,\n",
       "                        0.0186, -0.0652,  0.3715,  0.1126,  0.1610,  0.2265, -0.0441,  0.0189,\n",
       "                        0.0274, -0.2776,  0.1697, -0.3602, -0.0073, -0.1736, -0.2448,  0.1964,\n",
       "                       -0.0960,  0.2732,  0.2198,  0.0263,  0.0867, -0.1012,  0.4662,  0.1536,\n",
       "                       -0.1559, -0.0682, -0.0037, -0.2091,  0.0389, -0.1996,  0.1408,  0.2521,\n",
       "                        0.2191,  0.2547,  0.1037, -0.3443, -0.0855, -0.0261,  0.2740, -0.0662,\n",
       "                        0.0972,  0.1901, -0.1077,  0.4714,  0.1459,  0.3820,  0.1960,  0.1249,\n",
       "                        0.1640, -0.0692,  0.0047,  0.0349,  0.2028, -0.0133,  0.2828, -0.2189,\n",
       "                        0.0234, -0.0461,  0.2037, -0.1178, -0.2597,  0.0038, -0.0440, -0.0432,\n",
       "                       -0.2253, -0.2168, -0.4148,  0.0252,  0.2987, -0.0460, -0.1003, -0.0060])),\n",
       "              ('rnn.weight_ih_l0_reverse',\n",
       "               tensor([[ 0.2233,  0.1917,  0.4359,  ..., -0.6014, -0.4882,  0.0862],\n",
       "                       [ 0.0499,  0.0944, -0.1763,  ..., -1.2532, -1.2798, -0.0455],\n",
       "                       [-0.2083,  0.2932, -0.1979,  ..., -0.3162, -0.2616, -0.0219],\n",
       "                       ...,\n",
       "                       [-0.0771, -0.1133,  0.2094,  ..., -0.0902, -0.1006, -0.0978],\n",
       "                       [-0.0997, -0.0338, -0.2726,  ..., -0.5761, -0.5646,  0.0428],\n",
       "                       [ 0.0304, -0.3620, -0.0397,  ..., -0.6012, -0.7959, -0.1115]])),\n",
       "              ('rnn.weight_hh_l0_reverse',\n",
       "               tensor([[ 0.2019,  0.6864, -0.5308,  ...,  0.0928, -0.4960, -0.2413],\n",
       "                       [-0.9005,  0.2995, -0.6578,  ...,  0.1088,  0.9326,  0.0805],\n",
       "                       [ 0.0675, -0.6694, -0.5695,  ..., -0.1282,  1.0392,  0.0676],\n",
       "                       ...,\n",
       "                       [ 0.1186, -0.3963,  0.4128,  ...,  0.1117, -0.0195, -0.4267],\n",
       "                       [ 0.0688,  0.3751, -0.3959,  ..., -0.2726,  0.2519,  0.3322],\n",
       "                       [ 0.3469,  0.1054, -0.4081,  ...,  0.5437, -0.1310, -0.3467]])),\n",
       "              ('rnn.bias_ih_l0_reverse',\n",
       "               tensor([ 0.1103, -0.0200,  0.3057, -0.1073,  0.0547, -0.7820,  0.0484,  0.4398,\n",
       "                        0.0789,  0.4145,  0.0474,  0.4508, -0.0634,  0.0580, -0.2301, -0.0532,\n",
       "                       -0.6580,  0.2987,  0.0354, -0.4525, -0.2428, -0.0108, -0.7496, -0.0211,\n",
       "                       -0.0693, -0.2566, -0.1237,  0.1183, -0.7942, -0.8297, -0.0597, -0.1456,\n",
       "                        0.6143,  0.2817, -0.5213, -0.1174, -0.1192, -0.2791, -0.0905, -0.2316,\n",
       "                       -0.3050, -0.1608,  0.6697,  0.2252, -0.4207, -0.2672, -0.4615,  0.1491,\n",
       "                       -0.0135,  0.1548, -0.2481,  0.0195, -0.1972, -0.1213, -0.0607, -0.2738,\n",
       "                        0.3378, -0.9651,  0.2428,  0.0170, -0.3918, -0.0274,  0.2799,  0.0382,\n",
       "                       -0.3791, -0.6742,  0.3217, -0.6075,  0.0147,  0.1872, -0.0741,  0.1576,\n",
       "                       -0.4729, -0.3540,  0.0649,  0.2728, -0.7225, -0.0703,  0.2550, -0.2787,\n",
       "                       -0.2014,  0.1459,  0.4913,  0.2476,  0.4152,  0.0274,  0.3290, -0.5414,\n",
       "                       -0.1272,  0.0817,  0.0639,  0.0778,  0.4311,  0.5741,  0.4076, -0.0556,\n",
       "                        0.2591, -0.0848,  0.0481,  0.0848,  0.0045,  0.0049, -0.1837, -0.1557,\n",
       "                        0.5046, -0.0361,  0.1869, -0.3559,  0.0834,  0.0542,  0.1481, -0.3657,\n",
       "                       -0.3015, -0.0430, -0.1720, -0.5765, -0.5836, -0.0420,  0.2588,  0.0125,\n",
       "                       -0.1742,  0.6195,  0.2267, -0.5622, -0.2229,  0.2610, -0.4325, -0.7753,\n",
       "                       -0.0718, -0.0149, -0.1924,  0.1680,  0.0500, -0.0304,  0.1149,  0.3021,\n",
       "                        0.1634,  0.2833, -0.0995, -0.2245, -0.0939, -0.2539, -0.1698,  0.2965,\n",
       "                       -0.1790, -0.0723,  0.0032, -0.0711,  0.3466, -0.0544,  0.0678, -0.0614,\n",
       "                        0.3598,  0.1424, -0.0110,  0.6703, -0.3834, -0.2291, -0.2509,  0.1466,\n",
       "                        0.2361,  0.2773, -0.0804, -0.0738,  0.1261, -0.1058,  0.1054, -0.1418,\n",
       "                        0.0915,  0.1352,  0.0811, -0.0509, -0.0576,  0.0717,  0.0534,  0.0400,\n",
       "                        0.1335,  0.0521,  0.1928,  0.2980,  0.0879,  0.3263, -0.2471, -0.1641,\n",
       "                        0.0632, -0.0222, -0.0139,  0.0057,  0.4277,  0.1646,  0.3859, -0.3084,\n",
       "                       -0.0058,  0.3591, -0.2619,  0.1520,  0.0404,  0.1786,  0.1408,  0.2035,\n",
       "                        0.2371,  0.1175,  0.1090, -0.2870,  0.0459, -0.0409, -0.0535, -0.1302,\n",
       "                       -0.0794, -0.2383, -0.3648, -0.2252, -0.2616, -0.2788, -0.2957, -0.1120,\n",
       "                       -0.1966, -0.0404, -0.1413, -0.0781, -0.1738, -0.0489, -0.3369, -0.1224,\n",
       "                       -0.0951,  0.0421, -0.3467,  0.2465, -0.4690,  0.0444,  0.0159, -0.0402,\n",
       "                       -0.1511, -0.1179, -0.1902,  0.0179, -0.2049, -0.1925, -0.1359, -0.0506,\n",
       "                       -0.1597,  0.1923,  0.3684, -0.1108,  0.1365,  0.3497, -0.0080, -0.0888,\n",
       "                       -0.3026, -0.0943, -0.3614,  0.0905, -0.2579, -0.0550,  0.0460, -0.1110])),\n",
       "              ('rnn.bias_hh_l0_reverse',\n",
       "               tensor([ 1.3857e-01, -1.4327e-04,  4.5267e-01, -1.2492e-01,  2.5114e-02,\n",
       "                       -6.2326e-01,  5.5225e-02,  3.2076e-01,  1.5698e-01,  3.9320e-01,\n",
       "                       -1.1855e-01,  5.9580e-01, -1.7041e-02, -6.5110e-02, -1.2836e-01,\n",
       "                       -1.0901e-01, -4.8415e-01,  1.2750e-01,  1.4881e-01, -4.3976e-01,\n",
       "                       -1.1277e-01,  4.9405e-02, -7.2926e-01, -6.8937e-02, -1.6887e-01,\n",
       "                       -3.5908e-01, -1.7675e-01,  1.5214e-01, -7.3372e-01, -6.5754e-01,\n",
       "                        3.4902e-03, -2.9471e-01,  4.5999e-01,  2.3177e-01, -6.3880e-01,\n",
       "                       -3.5727e-01, -1.9357e-01, -2.5057e-01, -1.7753e-01, -1.5542e-01,\n",
       "                       -1.3584e-01, -1.9914e-01,  5.6088e-01,  9.4533e-02, -3.8042e-01,\n",
       "                       -1.4229e-01, -3.4330e-01,  1.4883e-01,  3.2860e-03,  1.6890e-01,\n",
       "                       -1.8305e-01, -4.3825e-02, -4.6404e-02, -2.3256e-01, -9.6431e-03,\n",
       "                       -9.8526e-02,  3.3994e-01, -8.3681e-01,  2.0735e-01,  1.4080e-01,\n",
       "                       -3.0238e-01,  1.1485e-01,  2.3220e-01,  2.4723e-03, -3.3273e-01,\n",
       "                       -5.0472e-01,  3.3561e-01, -6.5980e-01, -2.1314e-01,  1.5125e-01,\n",
       "                       -1.2527e-01,  2.0925e-01, -5.1820e-01, -1.4363e-01,  6.4216e-02,\n",
       "                        2.8594e-01, -5.9025e-01, -4.5021e-02,  1.6437e-02, -4.2620e-01,\n",
       "                        2.2256e-02,  4.3215e-02,  3.1745e-01,  3.0527e-01,  3.9210e-01,\n",
       "                       -1.3830e-02,  5.0646e-01, -5.9728e-01, -7.0530e-02,  3.2103e-02,\n",
       "                       -1.8488e-02,  1.4015e-01,  4.8624e-01,  4.4580e-01,  3.2050e-01,\n",
       "                       -2.5124e-01,  1.5216e-01, -5.9492e-02,  1.7355e-01, -3.0890e-02,\n",
       "                       -1.8036e-01,  5.3939e-02, -1.5666e-01, -2.5849e-01,  3.7961e-01,\n",
       "                       -1.1639e-03,  2.9247e-01, -3.4116e-01, -1.5163e-02,  2.9089e-02,\n",
       "                        2.2179e-01, -1.7911e-01, -3.5739e-01, -1.4120e-01, -2.0155e-01,\n",
       "                       -5.8896e-01, -6.3417e-01, -2.8697e-02,  3.8338e-01, -3.1272e-02,\n",
       "                       -1.2520e-01,  6.2797e-01,  3.1860e-01, -5.3860e-01, -1.8814e-01,\n",
       "                        2.0833e-01, -4.8120e-01, -7.7930e-01, -9.4367e-02,  2.1561e-02,\n",
       "                       -1.9446e-01,  9.1892e-02, -1.2683e-01, -1.5286e-01,  1.0251e-01,\n",
       "                        4.9037e-01,  1.6437e-01,  2.8088e-01, -2.3805e-01, -3.6023e-01,\n",
       "                       -2.2156e-02, -3.1618e-01, -1.1533e-01,  3.1375e-01, -1.3197e-01,\n",
       "                        3.7891e-02,  7.4130e-02, -1.2222e-01,  4.7716e-01,  1.6500e-01,\n",
       "                        2.3621e-01,  6.4506e-02,  1.7504e-01, -2.4170e-02,  1.0199e-01,\n",
       "                        8.2074e-01, -3.8042e-01, -2.4554e-01, -3.1046e-01,  1.1136e-01,\n",
       "                        2.7225e-01,  1.7732e-01, -1.5795e-02, -1.9885e-01, -2.6093e-02,\n",
       "                       -3.1547e-01, -1.0734e-03, -1.1064e-01,  9.0269e-02,  2.0180e-01,\n",
       "                        7.6346e-02, -2.3871e-01, -6.7938e-02,  2.2078e-04,  9.9935e-03,\n",
       "                        5.9160e-02,  1.4146e-01,  7.6185e-02,  3.5831e-01,  2.2863e-01,\n",
       "                       -1.1843e-01,  2.7335e-01, -2.4989e-01, -3.1158e-01,  7.6493e-02,\n",
       "                        1.0462e-01,  8.9610e-02, -1.6190e-01,  5.4016e-01,  5.5751e-02,\n",
       "                        2.6643e-01, -2.8900e-01,  6.9253e-02,  3.1598e-01, -1.1020e-01,\n",
       "                        1.5750e-01,  5.8260e-02,  6.1016e-02,  1.3102e-01,  2.0664e-01,\n",
       "                        9.1445e-02,  1.7743e-01,  2.1898e-01, -3.8125e-01, -1.1131e-01,\n",
       "                       -7.5892e-03, -2.1724e-01,  3.5098e-02, -1.2422e-01, -7.5157e-02,\n",
       "                       -2.1120e-01, -3.2874e-01, -3.1346e-01, -4.0634e-01, -1.7271e-01,\n",
       "                       -4.8204e-02, -2.9171e-01,  1.3874e-02, -1.7988e-01, -5.7827e-02,\n",
       "                       -2.4176e-01, -2.5568e-02, -2.9359e-01, -2.2720e-01, -2.3035e-01,\n",
       "                        1.1454e-01, -3.9594e-01,  1.2653e-01, -6.2932e-01,  3.3252e-02,\n",
       "                       -1.2068e-02, -7.9757e-02,  4.2565e-02, -1.1331e-01, -3.2330e-01,\n",
       "                       -1.3356e-01, -2.3725e-01, -2.6872e-01, -1.4026e-02, -1.5866e-01,\n",
       "                       -1.1375e-01,  3.3571e-01,  2.7657e-01, -4.1214e-02,  4.8168e-02,\n",
       "                        2.6363e-01, -9.9002e-02,  1.3920e-01, -2.0809e-01, -1.3133e-01,\n",
       "                       -4.5111e-01,  1.7251e-01, -1.1873e-01, -5.8428e-02,  1.5722e-01,\n",
       "                       -1.1927e-01])),\n",
       "              ('tok_clf.weight',\n",
       "               tensor([[ 0.0417,  0.0927, -0.0418, -0.1126, -0.0130, -0.0646, -0.0125, -0.0113,\n",
       "                         0.0453,  0.0511,  0.0551, -0.0091,  0.0663,  0.0788, -0.0422,  0.0357,\n",
       "                        -0.0124,  0.0741,  0.0954,  0.1300,  0.0647, -0.0142,  0.1194, -0.0965,\n",
       "                         0.0537, -0.0147,  0.0032,  0.0298,  0.0423,  0.0374,  0.0543,  0.1137,\n",
       "                         0.0100,  0.0169, -0.0808, -0.0584, -0.0626, -0.0058,  0.1287,  0.1084,\n",
       "                         0.0035, -0.0533,  0.0243,  0.0578,  0.0551, -0.0525, -0.0018, -0.0539,\n",
       "                         0.0818,  0.0385,  0.0122, -0.0941, -0.0187, -0.0225,  0.0880, -0.0068,\n",
       "                         0.0172,  0.0145, -0.1467, -0.0883, -0.1056,  0.0892,  0.1144, -0.0625,\n",
       "                         0.0066, -0.0798, -0.0639, -0.1006, -0.0963, -0.0784, -0.0597,  0.0282,\n",
       "                         0.0390, -0.0088,  0.0763,  0.0063, -0.0365,  0.0452, -0.0412,  0.0044,\n",
       "                        -0.0069,  0.0851, -0.0729,  0.1023, -0.0336, -0.1499,  0.0932,  0.0658,\n",
       "                        -0.0029, -0.0207, -0.0887, -0.0180,  0.0066,  0.0325,  0.0593,  0.0201,\n",
       "                         0.0953, -0.0613,  0.0731,  0.1138,  0.0534,  0.1895,  0.0631, -0.0149,\n",
       "                         0.0376, -0.0458, -0.0086,  0.1280, -0.0781,  0.0615, -0.1122, -0.2128,\n",
       "                         0.0373,  0.0485, -0.0315,  0.0122, -0.0792,  0.0043, -0.0838,  0.0295,\n",
       "                         0.0521, -0.0176, -0.2195, -0.0536,  0.0038, -0.0317, -0.0566, -0.1179]])),\n",
       "              ('tok_clf.bias', tensor([0.3273])),\n",
       "              ('sent_clf.weight',\n",
       "               tensor([[ 2.2123e-02, -1.0840e-01,  7.0985e-02, -2.2738e-01, -7.5774e-02,\n",
       "                        -5.3651e-02,  1.4258e-01, -7.9115e-02,  4.3926e-02, -6.9953e-02,\n",
       "                        -1.3318e-01,  8.9484e-02,  4.9372e-02,  1.3979e-02, -1.2207e-02,\n",
       "                         8.7252e-02,  1.3636e-01, -3.9579e-02, -9.5002e-02, -1.2791e-01,\n",
       "                         1.3318e-02,  2.9223e-02,  5.1786e-02,  5.4255e-02,  1.0905e-02,\n",
       "                         1.5016e-01, -7.8924e-02, -1.8006e-02,  1.8702e-01,  1.7321e-01,\n",
       "                         1.1039e-01, -6.4396e-02,  2.8766e-02, -1.9467e-01, -1.1958e-01,\n",
       "                        -2.0250e-03,  7.4127e-02,  2.2789e-02,  9.4595e-02,  1.9696e-02,\n",
       "                         5.6653e-02,  4.7499e-02,  1.8310e-02,  9.8542e-02, -4.9398e-02,\n",
       "                        -1.0113e-01, -5.4942e-02,  3.3107e-01,  2.7989e-02, -1.0466e-01,\n",
       "                        -1.7226e-02,  3.5695e-02,  4.4059e-02,  1.8891e-01,  6.0149e-03,\n",
       "                        -2.2606e-02, -3.9756e-02, -9.7885e-02, -2.4796e-01, -2.0884e-01,\n",
       "                        -4.5400e-02, -2.0274e-02, -2.6461e-01, -1.4528e-01, -1.4998e-02,\n",
       "                         3.7772e-02, -7.1790e-02,  2.0133e-02,  3.2448e-02,  7.2420e-02,\n",
       "                         2.7463e-01, -3.1844e-02,  3.2888e-02, -9.5193e-04,  2.1691e-01,\n",
       "                         1.0953e+00,  4.1727e-02,  3.7831e-02,  4.4086e-02, -4.8285e-02,\n",
       "                        -2.3168e-03, -2.0233e-01, -8.9763e-01, -1.1205e-02, -2.4158e-02,\n",
       "                        -4.3028e-01,  2.2493e-02, -2.0490e-02,  2.1828e-01, -1.9749e-01,\n",
       "                         2.0734e-02,  8.8586e-03, -3.4721e-01,  1.3255e-01,  9.5891e-03,\n",
       "                        -4.6944e-01,  1.4291e-01,  1.2958e-01,  2.1750e-01, -2.9963e-01,\n",
       "                        -1.2487e-01,  2.0238e-01,  2.1476e-02, -5.0317e-02,  3.2059e-01,\n",
       "                        -2.2214e-02,  5.8465e-01,  1.8287e-01, -5.5141e-01,  2.0969e-01,\n",
       "                         4.2430e-03,  3.2894e-02,  6.3428e-02, -4.5980e-01, -3.9141e-02,\n",
       "                        -1.5956e-01, -1.1055e-01, -3.1357e-01, -7.3115e-02, -4.3347e-01,\n",
       "                        -1.2473e-01, -1.3613e-01,  7.4688e-02,  5.6584e-02, -6.4932e-02,\n",
       "                         6.0559e-02, -6.3948e-02,  1.2574e-02]])),\n",
       "              ('sent_clf.bias', tensor([-0.6337])),\n",
       "              ('rnn2.weight_ih_l0',\n",
       "               tensor([[-0.4660,  0.0346,  0.3098,  ...,  0.0396,  0.5356, -0.1276],\n",
       "                       [-0.7213, -0.7967, -0.2618,  ..., -0.5326, -0.2745, -0.4555],\n",
       "                       [ 0.3375, -0.2914,  0.2897,  ...,  0.0316, -0.1470,  0.0414],\n",
       "                       ...,\n",
       "                       [ 0.1025, -0.4694,  0.2561,  ...,  0.2933,  0.0601, -0.1058],\n",
       "                       [ 0.5089,  0.4553,  0.2497,  ..., -0.0597, -0.2854, -0.1648],\n",
       "                       [-0.5240,  0.0478,  0.1758,  ..., -0.2145,  0.0887, -0.4298]])),\n",
       "              ('rnn2.weight_hh_l0',\n",
       "               tensor([[ 0.1530, -0.4779,  0.0889,  ...,  0.0404,  0.4158,  0.8710],\n",
       "                       [ 0.2681, -0.1285, -0.3885,  ..., -0.8474,  0.1105,  0.4510],\n",
       "                       [-0.2122, -0.3463, -0.0383,  ...,  0.2801, -0.0361, -0.2292],\n",
       "                       ...,\n",
       "                       [-0.6204, -0.6082, -0.1286,  ...,  0.7159,  0.1151,  0.1109],\n",
       "                       [ 0.1499,  0.2970,  0.4492,  ...,  0.1277, -0.4027,  0.3381],\n",
       "                       [ 0.4000, -0.1276, -0.0912,  ..., -0.1550,  0.0731, -0.6260]])),\n",
       "              ('rnn2.bias_ih_l0',\n",
       "               tensor([-2.3058e-01, -3.9010e-01, -5.0370e-01, -1.9233e-02,  2.1700e-01,\n",
       "                       -3.1938e-01, -1.8215e-01, -7.6281e-01, -5.5733e-01, -1.9290e-01,\n",
       "                       -4.3508e-02, -7.9206e-02,  1.6195e-01, -2.4476e-01, -7.7035e-02,\n",
       "                        4.5210e-03, -1.6256e-01, -1.1922e-01,  1.6235e-01,  1.9157e-01,\n",
       "                       -5.6323e-01, -2.0371e-01, -3.6482e-01, -2.7075e-01,  4.1746e-01,\n",
       "                       -2.3762e-01,  1.5057e-01, -2.1248e-01,  3.1857e-01, -6.8575e-01,\n",
       "                       -6.3737e-01,  2.4706e-01,  2.7156e-01, -5.5811e-01, -2.8262e-02,\n",
       "                       -1.0273e-01,  1.9006e-01, -1.7946e-01,  7.4758e-02, -4.3615e-01,\n",
       "                       -3.1173e-01,  3.2101e-01, -4.7917e-01, -2.5559e-01, -2.2748e-02,\n",
       "                       -3.8507e-01, -9.7797e-02, -6.0558e-02, -2.0870e-01, -7.8147e-01,\n",
       "                       -5.8761e-02, -2.1152e-01, -1.8072e-01, -2.1933e-01, -9.2964e-01,\n",
       "                       -7.3830e-02,  1.7223e-01, -2.8694e-01, -3.2565e-02, -4.6087e-01,\n",
       "                       -8.9743e-02, -2.1476e-01, -3.1684e-01, -2.8596e-01, -4.0881e-02,\n",
       "                        5.1083e-01, -3.9907e-02,  4.2379e-02, -3.8502e-01, -2.5096e-01,\n",
       "                        3.9949e-03,  5.6601e-01,  4.5187e-01, -5.2606e-01, -1.3728e-01,\n",
       "                       -5.8604e-01, -1.0318e-01, -8.6062e-02,  1.8752e-01, -5.6362e-02,\n",
       "                        2.7595e-01, -3.0354e-01, -3.3799e-01, -7.8864e-02,  2.3258e-01,\n",
       "                       -2.5802e-01,  1.1166e-01, -9.8536e-01, -2.7475e-01, -5.1196e-01,\n",
       "                       -3.9399e-02,  3.2500e-01, -2.1369e-01,  5.3219e-01,  2.4335e-01,\n",
       "                       -1.6867e-02, -2.3343e-01, -5.6445e-01, -4.5163e-01,  4.2109e-01,\n",
       "                       -1.4730e-01, -1.4204e-01,  6.8723e-02, -8.8959e-02, -1.5408e-01,\n",
       "                       -2.2387e-01, -5.8486e-01,  7.6943e-02, -2.2193e-01, -4.8553e-01,\n",
       "                        3.3593e-01, -1.1608e-01,  2.3450e-01,  6.0556e-01,  1.1929e-01,\n",
       "                        1.9960e-02,  3.6562e-01,  1.5086e-01,  4.4385e-01,  2.0951e-01,\n",
       "                        4.5356e-01, -2.9313e-01, -1.1401e-01,  4.7179e-01, -1.7537e-02,\n",
       "                        3.5503e-02,  1.6128e-01,  4.4963e-02, -3.2134e-01, -1.9146e-01,\n",
       "                       -7.0984e-02, -1.6339e-01, -1.1462e-01,  4.0407e-02, -4.6530e-01,\n",
       "                        2.5248e-01,  1.3574e-01, -5.2610e-03,  1.0849e-01, -1.7435e-01,\n",
       "                        3.3622e-01, -7.3424e-02, -9.1724e-02, -3.5219e-01,  3.0093e-01,\n",
       "                       -2.9948e-01,  4.5445e-02, -1.3270e-01, -2.5280e-01, -7.7755e-02,\n",
       "                        2.2351e-01, -6.5747e-02, -8.5463e-02, -4.2201e-02, -7.7684e-02,\n",
       "                       -1.9202e-02, -1.0214e-01, -9.2709e-03,  4.0197e-01,  1.9394e-01,\n",
       "                        2.0687e-01,  1.0831e-01,  1.1889e-01, -1.4975e-01,  1.6726e-01,\n",
       "                        7.6257e-02,  1.6494e-01, -1.7130e-01,  3.6360e-04, -8.5454e-02,\n",
       "                        1.1420e-01, -2.8052e-02,  6.1346e-03, -1.1487e-02, -1.0875e-01,\n",
       "                        1.9823e-02,  6.8963e-02, -1.3491e-01, -4.0574e-02, -9.5698e-02,\n",
       "                        1.1369e-01, -1.2755e-01,  2.3251e-01,  3.1431e-01, -4.1767e-01,\n",
       "                       -6.6830e-02, -4.5375e-01,  1.0936e-01,  1.4139e-02,  2.5879e-01,\n",
       "                       -1.4828e-01, -2.4466e-01,  2.0656e-01, -1.3295e-01, -1.5594e-01,\n",
       "                        3.3118e-02,  3.0834e-01,  1.9840e-02,  2.2397e-01, -1.0959e-01,\n",
       "                       -4.0665e-02, -1.6591e-01, -3.0933e-02, -4.1910e-01, -2.6862e-01,\n",
       "                       -2.7085e-01, -2.3927e-01,  6.2546e-02, -3.3886e-01, -6.6386e-02,\n",
       "                        5.1040e-02,  3.8957e-01, -3.4951e-01, -2.3363e-01, -3.1550e-01,\n",
       "                       -2.9476e-01, -9.2711e-02, -8.0489e-02, -5.7534e-02, -1.5157e-01,\n",
       "                        4.9098e-02,  1.9977e-01, -1.2415e-01, -3.5222e-01,  3.3704e-02,\n",
       "                       -8.9555e-02,  1.8786e-01,  8.6265e-02,  6.4153e-02, -2.9549e-01,\n",
       "                        1.3456e-01, -3.0458e-01, -1.1667e-01,  1.4506e-01, -3.5473e-01,\n",
       "                       -1.7944e-01, -3.8026e-01, -2.2128e-01, -4.6039e-02,  1.7260e-01,\n",
       "                       -3.1796e-01, -2.0732e-02,  1.5119e-01, -3.1843e-01, -1.4286e-01,\n",
       "                        3.1486e-02, -1.2839e-01, -1.8284e-01, -3.2904e-01, -3.2654e-01,\n",
       "                        1.9802e-02, -2.3597e-02, -5.2673e-02, -6.2947e-02, -4.0540e-02,\n",
       "                       -6.1830e-05])),\n",
       "              ('rnn2.bias_hh_l0',\n",
       "               tensor([-0.0366, -0.4539, -0.3295, -0.1476,  0.0273, -0.3485, -0.2048, -0.5447,\n",
       "                       -0.6870, -0.1856,  0.0191,  0.1186,  0.1996, -0.1195, -0.0574, -0.2043,\n",
       "                       -0.1215,  0.1198,  0.2402,  0.2226, -0.4571, -0.3303, -0.4150, -0.2924,\n",
       "                        0.3198, -0.2221,  0.1685, -0.1317,  0.5354, -0.7038, -0.4826,  0.2542,\n",
       "                        0.0771, -0.4159,  0.1236, -0.1718,  0.2742, -0.1467,  0.0833, -0.3309,\n",
       "                       -0.3160,  0.2068, -0.5191, -0.2685,  0.0118, -0.2593,  0.0516, -0.1078,\n",
       "                       -0.2247, -0.8208, -0.1318, -0.2134, -0.3809, -0.2202, -0.9125, -0.1005,\n",
       "                        0.1410, -0.2728, -0.1799, -0.5332, -0.0654, -0.2452, -0.3809, -0.2909,\n",
       "                       -0.0264,  0.5367, -0.0174,  0.1738, -0.1798, -0.0893,  0.2170,  0.5837,\n",
       "                        0.4131, -0.5593, -0.1208, -0.4826,  0.0156, -0.1458,  0.0721, -0.0794,\n",
       "                        0.4109, -0.1559, -0.2115, -0.0744,  0.3152, -0.1077,  0.1363, -0.8651,\n",
       "                       -0.1920, -0.6282,  0.1151,  0.3466, -0.2489,  0.4901,  0.2269, -0.0157,\n",
       "                       -0.1254, -0.7343, -0.3380,  0.3380, -0.2370, -0.0220, -0.0624,  0.0043,\n",
       "                       -0.0821, -0.0545, -0.4222,  0.1220, -0.1793, -0.4286,  0.1237, -0.0967,\n",
       "                        0.1690,  0.5863,  0.0174,  0.0135,  0.2876,  0.3297,  0.3594,  0.2714,\n",
       "                        0.6736, -0.2993, -0.0635,  0.5097, -0.0851, -0.0183,  0.2673,  0.0891,\n",
       "                       -0.3443, -0.1133, -0.0496, -0.0924, -0.2177,  0.1792, -0.4215,  0.1379,\n",
       "                        0.1582, -0.0880,  0.0181, -0.2016,  0.2106, -0.1021, -0.0401, -0.4906,\n",
       "                        0.1023, -0.2352,  0.1576, -0.2109, -0.1298, -0.2659,  0.2188,  0.1054,\n",
       "                       -0.1208, -0.1824, -0.0293, -0.1278, -0.0916, -0.0308,  0.5142,  0.0529,\n",
       "                        0.1358, -0.0526,  0.1362, -0.0846,  0.0221, -0.0736,  0.1563, -0.1588,\n",
       "                        0.0028, -0.2104,  0.1218,  0.0467,  0.0870, -0.0037,  0.0258,  0.2069,\n",
       "                        0.0363, -0.1207, -0.0309,  0.1080,  0.0519, -0.1818,  0.1376,  0.2416,\n",
       "                       -0.4573, -0.0928, -0.2650,  0.2044, -0.0494,  0.2124, -0.0578, -0.2861,\n",
       "                        0.0502, -0.0695, -0.1491, -0.0809,  0.3710, -0.0126,  0.2871, -0.0238,\n",
       "                       -0.0739,  0.0115,  0.0152, -0.5218, -0.2207, -0.2388, -0.0948, -0.0716,\n",
       "                       -0.2011,  0.0303,  0.1011,  0.2078, -0.4143, -0.2772, -0.0987, -0.3637,\n",
       "                       -0.0442, -0.2664, -0.1739, -0.1710,  0.0591,  0.0126, -0.0880, -0.1882,\n",
       "                        0.0215, -0.1600,  0.0425,  0.2423,  0.1296, -0.3691,  0.2105, -0.1431,\n",
       "                       -0.1569,  0.0972, -0.3705, -0.3527, -0.3094, -0.3102, -0.1102,  0.0372,\n",
       "                       -0.1748, -0.2179,  0.1825, -0.3440, -0.0034,  0.0702,  0.1101, -0.1752,\n",
       "                       -0.3567, -0.2657,  0.0825,  0.1104, -0.1651, -0.0497, -0.0525,  0.0011])),\n",
       "              ('rnn2.weight_ih_l0_reverse',\n",
       "               tensor([[ 0.2173, -0.0267, -0.2364,  ..., -0.1530,  0.0367, -0.2508],\n",
       "                       [-0.3264, -0.5801, -0.2491,  ..., -0.2754, -0.2636,  0.1601],\n",
       "                       [-0.4904,  0.4051,  0.4743,  ...,  0.0114,  0.6976, -0.4671],\n",
       "                       ...,\n",
       "                       [-0.1866,  0.4379,  0.0884,  ...,  0.2011,  0.1768,  0.2512],\n",
       "                       [-0.6697,  0.0945,  0.1351,  ..., -0.1808,  0.0218,  0.3785],\n",
       "                       [ 0.0667, -0.5088,  0.3852,  ..., -0.0877,  0.5706, -0.0029]])),\n",
       "              ('rnn2.weight_hh_l0_reverse',\n",
       "               tensor([[-0.7089,  0.0515, -0.1969,  ...,  0.1161, -0.0733,  0.0145],\n",
       "                       [ 0.6337, -0.2406,  1.1891,  ...,  0.6177,  0.0390, -1.1785],\n",
       "                       [ 0.2644,  0.3091,  0.3253,  ...,  0.1265, -0.6863, -0.1565],\n",
       "                       ...,\n",
       "                       [ 0.2313,  0.6266,  0.2277,  ..., -0.3430, -0.0575, -0.2133],\n",
       "                       [ 0.3410, -0.0626,  0.3884,  ..., -0.1703,  0.3881, -0.1789],\n",
       "                       [ 0.2892, -0.0924,  0.2197,  ..., -0.0189, -0.3355, -0.2016]])),\n",
       "              ('rnn2.bias_ih_l0_reverse',\n",
       "               tensor([-0.0620, -0.8638, -0.0469, -0.4925,  0.0554, -0.4867,  0.1127,  0.4827,\n",
       "                        0.1432, -0.8643,  0.1557, -0.4578, -0.5662, -0.3010,  0.5400, -0.7110,\n",
       "                       -0.3806, -0.6754,  0.1370, -0.5321, -0.6429, -0.0776, -0.0081, -0.5579,\n",
       "                       -0.1649, -0.3578,  0.0495,  0.3529, -0.8040,  0.1141, -0.7561, -0.2940,\n",
       "                       -0.1010, -0.2791, -0.1753, -0.3102, -0.1933, -0.2360,  0.0567,  0.3146,\n",
       "                        0.6131,  0.1327, -0.7107,  0.4265,  0.0335, -0.0879, -0.3517,  0.0211,\n",
       "                       -0.1043, -0.3683, -0.3420, -0.3897, -0.4149, -0.0057,  0.1320, -1.3695,\n",
       "                       -0.1883,  0.1117,  0.8998,  0.0306,  0.1388,  0.1937, -0.4414, -0.8942,\n",
       "                       -0.2925, -0.7758,  0.1570, -0.2122,  0.0207, -0.3452,  0.2120, -0.3015,\n",
       "                        0.1854, -0.7996, -0.0822, -0.1302,  0.2491,  0.2642, -0.2271,  0.3547,\n",
       "                        0.0871,  0.3542,  0.0538,  0.2284,  0.4495, -0.0155,  0.0852,  0.2199,\n",
       "                       -0.0917,  0.2377, -0.2649, -0.0427, -0.6559, -0.0523,  0.4439, -0.4548,\n",
       "                        0.1639,  0.0942,  0.1578, -0.0150,  0.1813,  0.0517, -0.5050, -0.0321,\n",
       "                        0.3469, -0.1244,  0.2768,  0.3117,  0.1827,  0.1591, -0.1453, -0.0371,\n",
       "                        0.0066, -0.2958, -0.1751, -0.3441,  0.1810,  0.0665, -0.0119,  0.2730,\n",
       "                        0.5962, -0.2834, -0.1290,  0.0969,  0.1824,  0.0843, -0.6731, -0.4047,\n",
       "                        0.1520,  0.2021,  0.4312, -0.1227, -0.1371,  0.1944, -0.1009, -0.1838,\n",
       "                        0.0207, -0.1772,  0.2574,  0.4184,  0.1374, -0.0525,  0.2171,  0.0488,\n",
       "                        0.2542, -0.0277, -0.3156, -0.1383, -0.0905,  0.0361,  0.2101,  0.0875,\n",
       "                       -0.2242,  0.0754,  0.2696,  0.0222, -0.1335,  0.1024, -0.0705, -0.2717,\n",
       "                        0.0482, -0.2726,  0.1165,  0.2527,  0.5118, -0.1038,  0.0909,  0.4299,\n",
       "                        0.2837,  0.2448,  0.0283,  0.3591,  0.0694, -0.2653, -0.3486, -0.0783,\n",
       "                       -0.1323, -0.2282, -0.1394, -0.1749,  0.0229, -0.0565, -0.2595,  0.1511,\n",
       "                        0.2102,  0.1526,  0.9634, -0.1659,  0.0652,  0.1372,  0.0905,  0.1627,\n",
       "                        0.0657, -0.5122, -0.1856, -0.4887,  0.4331, -0.4706, -0.4707,  0.3633,\n",
       "                        0.0771, -0.6853,  0.0799, -0.1918, -0.1384, -0.2573,  0.1348,  0.0611,\n",
       "                       -0.1739, -0.2460,  0.0999, -0.1619,  0.0910,  0.0228, -0.1512, -0.2510,\n",
       "                       -0.2208, -0.3387, -0.1634, -0.2803,  0.1946,  0.0029, -0.2359, -0.6212,\n",
       "                       -0.1371, -0.3098,  0.3284, -0.6184, -0.2019, -0.0143, -0.3105,  0.9343,\n",
       "                        0.3409,  0.2484, -0.4028, -0.3861, -0.3209, -0.3033, -0.2249,  0.1375,\n",
       "                       -0.0037, -0.6609, -0.2724, -0.8804, -0.4017, -0.2811,  0.0048, -0.2863,\n",
       "                       -0.1533, -0.5307,  0.4862, -0.0896,  0.1060, -0.2066, -0.9465, -0.3854])),\n",
       "              ('rnn2.bias_hh_l0_reverse',\n",
       "               tensor([-0.0165, -0.7019, -0.2346, -0.6523,  0.0584, -0.2955,  0.1745,  0.4043,\n",
       "                        0.1292, -0.8471,  0.0925, -0.4770, -0.5288, -0.1024,  0.5896, -0.6615,\n",
       "                       -0.2812, -0.6113,  0.1027, -0.5566, -0.5746, -0.0289,  0.1239, -0.5813,\n",
       "                       -0.1465, -0.1673,  0.0162,  0.3229, -0.8005,  0.1436, -0.8163, -0.2249,\n",
       "                       -0.1095, -0.2214, -0.1721, -0.4404, -0.0926, -0.1464,  0.0965,  0.3554,\n",
       "                        0.5181,  0.2723, -0.5665,  0.4178, -0.1338,  0.0556, -0.3891, -0.1082,\n",
       "                       -0.2770, -0.1855, -0.4554, -0.5651, -0.5431, -0.1721,  0.1412, -1.2662,\n",
       "                       -0.2109,  0.1618,  0.8222, -0.0510,  0.1506,  0.3959, -0.4177, -0.8797,\n",
       "                       -0.2585, -0.7362,  0.1591, -0.2684, -0.1820, -0.2646,  0.1675, -0.2806,\n",
       "                       -0.0275, -0.7390, -0.1837, -0.1080,  0.1658,  0.1115, -0.2697,  0.3420,\n",
       "                        0.1595,  0.2747,  0.0539,  0.3027,  0.2888,  0.1881,  0.1282,  0.0880,\n",
       "                        0.0940,  0.2069, -0.3163, -0.0574, -0.8708, -0.0764,  0.4683, -0.5423,\n",
       "                        0.1368,  0.0236,  0.1184,  0.0104,  0.1299, -0.0048, -0.6264, -0.1610,\n",
       "                        0.4757, -0.2030,  0.1240,  0.2672,  0.3522,  0.2540, -0.1853,  0.1690,\n",
       "                        0.1399, -0.3110, -0.0513, -0.1971,  0.0405,  0.0376, -0.1399,  0.2530,\n",
       "                        0.4424, -0.2778, -0.1770, -0.0402,  0.1499,  0.2393, -0.5910, -0.3170,\n",
       "                        0.0464,  0.1319,  0.3476, -0.1372, -0.2943,  0.1910, -0.0030, -0.0733,\n",
       "                        0.0322, -0.0223,  0.1830,  0.2420,  0.1142,  0.0206,  0.2294,  0.1065,\n",
       "                        0.1004, -0.0559, -0.3710,  0.0488, -0.0704,  0.2330,  0.2883,  0.2607,\n",
       "                       -0.0464, -0.0621,  0.1225,  0.0740,  0.0231,  0.1502, -0.1605, -0.0557,\n",
       "                       -0.1026, -0.2147,  0.1342,  0.1786,  0.3796, -0.3380,  0.2086,  0.3048,\n",
       "                        0.4213,  0.2525,  0.0743,  0.1819,  0.2115, -0.3174, -0.2828, -0.0538,\n",
       "                       -0.2194, -0.1467, -0.1828,  0.0264, -0.0321,  0.0533, -0.2709,  0.0759,\n",
       "                        0.1047,  0.1422,  0.9645, -0.0792, -0.0451,  0.1380,  0.1569,  0.1958,\n",
       "                       -0.1138, -0.3516, -0.3392, -0.4345,  0.3751, -0.6783, -0.4966,  0.3952,\n",
       "                        0.0637, -0.6648,  0.0165, -0.1085, -0.1274, -0.3296,  0.2525, -0.0421,\n",
       "                       -0.3064, -0.2589,  0.0756, -0.2690,  0.0471,  0.0191, -0.3007, -0.0999,\n",
       "                       -0.0703, -0.2370, -0.2881, -0.2696,  0.2312,  0.0764, -0.2240, -0.5647,\n",
       "                       -0.1333, -0.2143,  0.2439, -0.5369, -0.3584, -0.1403, -0.1368,  0.9350,\n",
       "                        0.4211,  0.1345, -0.3110, -0.5117, -0.1819, -0.4446, -0.2035,  0.0932,\n",
       "                       -0.0586, -0.6589, -0.2050, -0.8508, -0.2872, -0.1950,  0.2156, -0.2749,\n",
       "                       -0.2298, -0.6784,  0.6333, -0.1670, -0.1188, -0.3459, -1.1540, -0.3218])),\n",
       "              ('tok_clf2.weight',\n",
       "               tensor([[ 7.5707e-02, -2.5025e-02, -8.9630e-01, -1.0287e-01,  9.0410e-02,\n",
       "                        -3.1863e-01,  1.6491e-01, -1.8958e-01, -1.7846e-01,  2.0810e+00,\n",
       "                        -4.9836e-02,  1.0663e-01, -1.7104e-01, -1.1487e+00, -1.8048e-01,\n",
       "                         1.2350e+00, -2.3262e-01,  9.6964e-02,  1.9126e-01,  2.6245e-02,\n",
       "                         1.6838e-01, -6.0747e-02, -4.3405e-01, -2.2709e+00,  5.9113e-01,\n",
       "                         1.2812e+00, -1.7740e-01,  5.5157e-02,  5.5214e-02, -5.1285e-04,\n",
       "                         4.5455e-04, -3.2630e-01,  4.9644e-01,  1.7286e-01, -1.1869e+00,\n",
       "                        -7.6068e-02,  4.5143e-02,  2.4467e-01, -1.4511e-01,  5.5436e-03,\n",
       "                         1.5382e+00,  6.5550e-01,  2.9056e+00, -1.1721e-01,  3.5338e-01,\n",
       "                        -2.2544e+00,  1.2985e+00,  1.1288e+00, -1.7442e-01, -1.4833e-01,\n",
       "                         2.1060e-02,  1.3684e-01, -2.3067e-02,  3.0295e-02,  5.4963e-04,\n",
       "                        -2.0595e-02,  3.3800e-01, -4.2908e-01, -1.5089e-01, -5.2589e-02,\n",
       "                         4.9589e-01, -2.1922e-01,  6.2414e-01,  5.4726e-01,  5.4516e-02,\n",
       "                         3.8953e+00, -4.7932e-01,  1.8662e+00,  2.1666e-02, -5.0492e-01,\n",
       "                         5.7980e-01, -2.8080e-01,  1.2510e-02, -5.1655e+00,  2.7511e-01,\n",
       "                        -8.2510e-01, -7.4979e-01,  6.7536e-01,  1.5362e-01, -6.4302e-01,\n",
       "                        -7.5730e-02, -9.4842e-02, -9.5317e-02,  5.7066e-02,  1.7997e+00,\n",
       "                        -1.2928e-01, -3.8667e-01, -9.1788e-01, -8.9144e-02, -7.7431e-01,\n",
       "                        -3.3896e-01,  3.4734e-01,  2.3476e+00, -1.1605e-01,  2.7473e-01,\n",
       "                         1.7293e+00, -9.6179e-01,  1.1897e+00, -2.7340e-01, -2.0686e+00,\n",
       "                        -3.0561e-01,  4.1337e-03, -1.9122e+00,  1.1065e-01,  5.3125e-02,\n",
       "                        -2.7249e-01, -1.1631e+00, -3.3060e-01,  2.0974e-01,  4.7044e-01,\n",
       "                         1.7624e+00,  2.2462e-01, -1.4202e-01,  1.5806e+00, -5.2432e-01,\n",
       "                        -3.0158e+00, -1.3828e+00, -1.7819e-01,  2.3377e-01,  2.2276e+00,\n",
       "                         8.5672e-01, -1.5032e+00, -3.3105e-02,  5.3560e-01,  4.3009e-01,\n",
       "                         3.4578e-01, -4.6313e+00,  3.7475e+00]])),\n",
       "              ('sent_clf2.weight',\n",
       "               tensor([[ 4.2422e-01,  2.1198e-01,  6.2665e-01,  4.8146e-01,  1.0355e-01,\n",
       "                        -3.4557e-01,  1.2856e-01, -2.9511e-01, -1.4301e-01, -1.2251e-01,\n",
       "                        -3.2248e-01, -1.5440e+00,  2.0760e-01,  1.1749e-02,  1.2304e-01,\n",
       "                         4.6795e-01, -1.3992e-01, -4.3590e-02, -1.0040e-01,  1.2517e-03,\n",
       "                         1.0849e-01,  2.7169e-01, -7.2028e-01, -7.8895e-03,  9.5058e-03,\n",
       "                         7.7959e-03, -1.7382e-02, -1.7677e-01, -2.0758e-01,  1.1641e-01,\n",
       "                         8.8043e-01,  1.3777e-01, -4.1490e-02, -2.0595e+00,  4.5218e-03,\n",
       "                        -2.8615e-01,  1.7020e-01,  9.0975e-01, -1.5148e-01,  5.2353e-01,\n",
       "                         3.0773e-01,  3.2282e-02, -6.9981e-02, -4.5542e-01, -6.6217e-01,\n",
       "                        -6.1861e-03,  2.6911e-01, -7.7085e-01, -1.1806e-02,  1.4025e-01,\n",
       "                         3.1430e-01, -4.7373e-01, -8.7902e-02,  3.5706e-01,  9.1127e-02,\n",
       "                        -5.1950e-01,  8.8815e-01, -4.4627e-02,  4.8971e-01, -5.5142e-01,\n",
       "                        -6.4754e-01, -7.8182e-01,  3.3290e-01,  5.0965e-01,  2.7179e-01,\n",
       "                        -7.4301e-03, -2.9234e-01, -4.6471e-01, -2.6816e-01,  5.6815e-01,\n",
       "                        -3.1258e-01,  7.5941e-02, -7.8501e-02, -1.0915e-02, -2.8421e-02,\n",
       "                        -1.0663e+00,  6.1939e-01,  2.5183e-01,  1.3283e-01, -8.6513e-01,\n",
       "                         1.5421e-01, -1.8295e-01,  9.6138e-02,  8.0433e-02,  4.8593e-01,\n",
       "                        -5.7335e-01,  1.0906e-01, -3.2984e-01, -6.8748e-02,  2.7673e-01,\n",
       "                        -1.0655e+00, -2.2713e-01,  1.0504e-02,  1.3059e-01,  2.6546e-01,\n",
       "                        -4.6924e-01,  3.4739e-02,  3.8636e-01, -3.5118e-01,  4.1038e-01,\n",
       "                         1.6937e-01, -1.9989e-01, -2.1143e-03,  1.8028e-04,  1.5808e-01,\n",
       "                        -1.8310e-02, -3.9434e-01, -5.6450e-01, -1.1417e-01,  8.3276e-01,\n",
       "                        -3.1342e-01,  5.9463e-01,  2.2800e-01, -4.1708e-01,  1.6054e+00,\n",
       "                        -2.0682e-02, -4.5335e-01,  1.3431e-01,  2.5189e-01,  1.8920e+00,\n",
       "                        -2.3132e+00, -1.2675e-01, -1.5378e-01,  1.3127e-01, -3.9015e-02,\n",
       "                        -1.0227e+00, -3.0417e-05, -4.1546e-01]]))]),\n",
       " 'vocab': OrderedDict([('lang', 'grc'),\n",
       "              ('idx', 0),\n",
       "              ('cutoff', 0),\n",
       "              ('lower', False),\n",
       "              ('_unit2id',\n",
       "               {' ': 2,\n",
       "                '.': 171,\n",
       "                '<PAD>': 0,\n",
       "                '<UNK>': 1,\n",
       "                'ʹ': 177,\n",
       "                'ΐ': 124,\n",
       "                'Α': 103,\n",
       "                'Β': 97,\n",
       "                'Γ': 106,\n",
       "                'Δ': 87,\n",
       "                'Ε': 114,\n",
       "                'Ζ': 127,\n",
       "                'Θ': 101,\n",
       "                'Κ': 65,\n",
       "                'Λ': 82,\n",
       "                'Μ': 74,\n",
       "                'Ν': 113,\n",
       "                'Ξ': 110,\n",
       "                'Ο': 128,\n",
       "                'Π': 67,\n",
       "                'Ρ': 174,\n",
       "                'Σ': 73,\n",
       "                'Τ': 91,\n",
       "                'Φ': 93,\n",
       "                'Χ': 80,\n",
       "                'Ψ': 158,\n",
       "                'ά': 27,\n",
       "                'έ': 19,\n",
       "                'ή': 40,\n",
       "                'ί': 21,\n",
       "                'ΰ': 149,\n",
       "                'α': 5,\n",
       "                'β': 41,\n",
       "                'γ': 20,\n",
       "                'δ': 16,\n",
       "                'ε': 7,\n",
       "                'ζ': 56,\n",
       "                'η': 23,\n",
       "                'θ': 24,\n",
       "                'ι': 8,\n",
       "                'κ': 12,\n",
       "                'λ': 15,\n",
       "                'μ': 14,\n",
       "                'ν': 3,\n",
       "                'ξ': 46,\n",
       "                'ο': 6,\n",
       "                'π': 13,\n",
       "                'ρ': 11,\n",
       "                'ς': 9,\n",
       "                'σ': 10,\n",
       "                'τ': 4,\n",
       "                'υ': 18,\n",
       "                'φ': 38,\n",
       "                'χ': 32,\n",
       "                'ψ': 66,\n",
       "                'ω': 25,\n",
       "                'ϊ': 111,\n",
       "                'ϋ': 126,\n",
       "                'ό': 30,\n",
       "                'ύ': 34,\n",
       "                'ώ': 50,\n",
       "                'ϛ': 176,\n",
       "                'ἀ': 31,\n",
       "                'ἁ': 84,\n",
       "                'ἂ': 96,\n",
       "                'ἃ': 119,\n",
       "                'ἄ': 54,\n",
       "                'ἅ': 92,\n",
       "                'ἆ': 147,\n",
       "                'Ἀ': 63,\n",
       "                'Ἁ': 143,\n",
       "                'Ἄ': 116,\n",
       "                'Ἅ': 141,\n",
       "                'ἐ': 17,\n",
       "                'ἑ': 64,\n",
       "                'ἓ': 125,\n",
       "                'ἔ': 44,\n",
       "                'ἕ': 85,\n",
       "                'Ἐ': 102,\n",
       "                'Ἑ': 99,\n",
       "                'Ἔ': 135,\n",
       "                'Ἕ': 112,\n",
       "                'ἠ': 90,\n",
       "                'ἡ': 51,\n",
       "                'ἢ': 88,\n",
       "                'ἣ': 118,\n",
       "                'ἤ': 83,\n",
       "                'ἥ': 105,\n",
       "                'ἦ': 72,\n",
       "                'ἧ': 122,\n",
       "                'Ἠ': 138,\n",
       "                'Ἡ': 115,\n",
       "                'Ἢ': 160,\n",
       "                'Ἤ': 152,\n",
       "                'Ἥ': 162,\n",
       "                'Ἦ': 145,\n",
       "                'ἰ': 42,\n",
       "                'ἱ': 49,\n",
       "                'ἲ': 165,\n",
       "                'ἳ': 107,\n",
       "                'ἴ': 70,\n",
       "                'ἵ': 69,\n",
       "                'ἶ': 60,\n",
       "                'ἷ': 98,\n",
       "                'Ἰ': 62,\n",
       "                'Ἱ': 104,\n",
       "                'Ἲ': 166,\n",
       "                'Ἴ': 120,\n",
       "                'Ἵ': 159,\n",
       "                'Ἶ': 169,\n",
       "                'ὀ': 75,\n",
       "                'ὁ': 45,\n",
       "                'ὂ': 173,\n",
       "                'ὃ': 79,\n",
       "                'ὄ': 78,\n",
       "                'ὅ': 53,\n",
       "                'Ὀ': 121,\n",
       "                'Ὁ': 142,\n",
       "                'Ὄ': 151,\n",
       "                'Ὅ': 144,\n",
       "                'ὐ': 29,\n",
       "                'ὑ': 47,\n",
       "                'ὒ': 157,\n",
       "                'ὓ': 133,\n",
       "                'ὔ': 89,\n",
       "                'ὕ': 76,\n",
       "                'ὖ': 81,\n",
       "                'ὗ': 77,\n",
       "                'Ὑ': 134,\n",
       "                'Ὕ': 156,\n",
       "                'ὠ': 123,\n",
       "                'ὡ': 71,\n",
       "                'ὢ': 137,\n",
       "                'ὣ': 154,\n",
       "                'ὤ': 129,\n",
       "                'ὥ': 94,\n",
       "                'ὦ': 95,\n",
       "                'ὧ': 108,\n",
       "                'Ὠ': 163,\n",
       "                'Ὡ': 150,\n",
       "                'Ὥ': 168,\n",
       "                'Ὦ': 164,\n",
       "                'ὰ': 36,\n",
       "                'ὲ': 33,\n",
       "                'ὴ': 39,\n",
       "                'ὶ': 22,\n",
       "                'ὸ': 28,\n",
       "                'ὺ': 52,\n",
       "                'ὼ': 68,\n",
       "                'ᾄ': 161,\n",
       "                'ᾅ': 155,\n",
       "                'ᾐ': 153,\n",
       "                'ᾑ': 170,\n",
       "                'ᾔ': 140,\n",
       "                'ᾖ': 132,\n",
       "                'ᾗ': 139,\n",
       "                'ᾠ': 148,\n",
       "                'ᾧ': 117,\n",
       "                'ᾳ': 86,\n",
       "                'ᾴ': 175,\n",
       "                'ᾶ': 61,\n",
       "                'ᾷ': 109,\n",
       "                'ῂ': 167,\n",
       "                'ῃ': 57,\n",
       "                'ῄ': 136,\n",
       "                'ῆ': 43,\n",
       "                'ῇ': 55,\n",
       "                'ῒ': 146,\n",
       "                'ῖ': 35,\n",
       "                'ῢ': 172,\n",
       "                'ῥ': 100,\n",
       "                'ῦ': 26,\n",
       "                'Ῥ': 131,\n",
       "                'ῳ': 59,\n",
       "                'ῴ': 130,\n",
       "                'ῶ': 37,\n",
       "                'ῷ': 48,\n",
       "                '’': 58}),\n",
       "              ('_id2unit',\n",
       "               ['<PAD>',\n",
       "                '<UNK>',\n",
       "                ' ',\n",
       "                'ν',\n",
       "                'τ',\n",
       "                'α',\n",
       "                'ο',\n",
       "                'ε',\n",
       "                'ι',\n",
       "                'ς',\n",
       "                'σ',\n",
       "                'ρ',\n",
       "                'κ',\n",
       "                'π',\n",
       "                'μ',\n",
       "                'λ',\n",
       "                'δ',\n",
       "                'ἐ',\n",
       "                'υ',\n",
       "                'έ',\n",
       "                'γ',\n",
       "                'ί',\n",
       "                'ὶ',\n",
       "                'η',\n",
       "                'θ',\n",
       "                'ω',\n",
       "                'ῦ',\n",
       "                'ά',\n",
       "                'ὸ',\n",
       "                'ὐ',\n",
       "                'ό',\n",
       "                'ἀ',\n",
       "                'χ',\n",
       "                'ὲ',\n",
       "                'ύ',\n",
       "                'ῖ',\n",
       "                'ὰ',\n",
       "                'ῶ',\n",
       "                'φ',\n",
       "                'ὴ',\n",
       "                'ή',\n",
       "                'β',\n",
       "                'ἰ',\n",
       "                'ῆ',\n",
       "                'ἔ',\n",
       "                'ὁ',\n",
       "                'ξ',\n",
       "                'ὑ',\n",
       "                'ῷ',\n",
       "                'ἱ',\n",
       "                'ώ',\n",
       "                'ἡ',\n",
       "                'ὺ',\n",
       "                'ὅ',\n",
       "                'ἄ',\n",
       "                'ῇ',\n",
       "                'ζ',\n",
       "                'ῃ',\n",
       "                '’',\n",
       "                'ῳ',\n",
       "                'ἶ',\n",
       "                'ᾶ',\n",
       "                'Ἰ',\n",
       "                'Ἀ',\n",
       "                'ἑ',\n",
       "                'Κ',\n",
       "                'ψ',\n",
       "                'Π',\n",
       "                'ὼ',\n",
       "                'ἵ',\n",
       "                'ἴ',\n",
       "                'ὡ',\n",
       "                'ἦ',\n",
       "                'Σ',\n",
       "                'Μ',\n",
       "                'ὀ',\n",
       "                'ὕ',\n",
       "                'ὗ',\n",
       "                'ὄ',\n",
       "                'ὃ',\n",
       "                'Χ',\n",
       "                'ὖ',\n",
       "                'Λ',\n",
       "                'ἤ',\n",
       "                'ἁ',\n",
       "                'ἕ',\n",
       "                'ᾳ',\n",
       "                'Δ',\n",
       "                'ἢ',\n",
       "                'ὔ',\n",
       "                'ἠ',\n",
       "                'Τ',\n",
       "                'ἅ',\n",
       "                'Φ',\n",
       "                'ὥ',\n",
       "                'ὦ',\n",
       "                'ἂ',\n",
       "                'Β',\n",
       "                'ἷ',\n",
       "                'Ἑ',\n",
       "                'ῥ',\n",
       "                'Θ',\n",
       "                'Ἐ',\n",
       "                'Α',\n",
       "                'Ἱ',\n",
       "                'ἥ',\n",
       "                'Γ',\n",
       "                'ἳ',\n",
       "                'ὧ',\n",
       "                'ᾷ',\n",
       "                'Ξ',\n",
       "                'ϊ',\n",
       "                'Ἕ',\n",
       "                'Ν',\n",
       "                'Ε',\n",
       "                'Ἡ',\n",
       "                'Ἄ',\n",
       "                'ᾧ',\n",
       "                'ἣ',\n",
       "                'ἃ',\n",
       "                'Ἴ',\n",
       "                'Ὀ',\n",
       "                'ἧ',\n",
       "                'ὠ',\n",
       "                'ΐ',\n",
       "                'ἓ',\n",
       "                'ϋ',\n",
       "                'Ζ',\n",
       "                'Ο',\n",
       "                'ὤ',\n",
       "                'ῴ',\n",
       "                'Ῥ',\n",
       "                'ᾖ',\n",
       "                'ὓ',\n",
       "                'Ὑ',\n",
       "                'Ἔ',\n",
       "                'ῄ',\n",
       "                'ὢ',\n",
       "                'Ἠ',\n",
       "                'ᾗ',\n",
       "                'ᾔ',\n",
       "                'Ἅ',\n",
       "                'Ὁ',\n",
       "                'Ἁ',\n",
       "                'Ὅ',\n",
       "                'Ἦ',\n",
       "                'ῒ',\n",
       "                'ἆ',\n",
       "                'ᾠ',\n",
       "                'ΰ',\n",
       "                'Ὡ',\n",
       "                'Ὄ',\n",
       "                'Ἤ',\n",
       "                'ᾐ',\n",
       "                'ὣ',\n",
       "                'ᾅ',\n",
       "                'Ὕ',\n",
       "                'ὒ',\n",
       "                'Ψ',\n",
       "                'Ἵ',\n",
       "                'Ἢ',\n",
       "                'ᾄ',\n",
       "                'Ἥ',\n",
       "                'Ὠ',\n",
       "                'Ὦ',\n",
       "                'ἲ',\n",
       "                'Ἲ',\n",
       "                'ῂ',\n",
       "                'Ὥ',\n",
       "                'Ἶ',\n",
       "                'ᾑ',\n",
       "                '.',\n",
       "                'ῢ',\n",
       "                'ὂ',\n",
       "                'Ρ',\n",
       "                'ᾴ',\n",
       "                'ϛ',\n",
       "                'ʹ'])])}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#inspect neural model (tokenize.pt for grc = ancient greek)\n",
    "import torch\n",
    "from os.path import expanduser\n",
    "home = expanduser(\"~\")\n",
    "torch.load(home+'/stanza_resources/grc/tokenize/proiel.pt',map_location=torch.device('cpu'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
