{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 3: Korpusannotation mit stanza\n",
    "\n",
    "- **stanza = Stanford Python NLP Tools**\n",
    "  - https://stanfordnlp.github.io/stanza/\n",
    "  - trainiert auf Universal-Dependencies-Korpora (**UD**):\n",
    "    - https://stanfordnlp.github.io/stanza/available_models.html\n",
    "  - verwendet ausschließlich neuronale Modelle\n",
    "\n",
    "- **Processors:**\n",
    "  - Tokenization\n",
    "  - Lemmatization\n",
    "  - POS Tagging\n",
    "  - Syntactic Parsing\n",
    "  - Named Entity Recognition\n",
    "  - Sentiment Analysis\n",
    "\n",
    "\n",
    "- https://www.nltk.org/book/ch05.html (Tagging)\n",
    "- https://www.nltk.org/book/ch06.html (Text Classification)\n",
    "- https://www.nltk.org/book/ch07.html (Information Extraction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part I: Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Automated Text Annotation\n",
    "\n",
    "#### = automatic analysis of linguistic data based on *NLP Classification Models* (Classifiers)\n",
    "\n",
    "- **Classification Task**: assign annotation class label to input\n",
    "  - **Tagging: word &rarr; annotation label**\n",
    "      - e.g. POS-label to each word in a sentence/text\n",
    "  - **Parsing: sentence &rarr; tree structure**\n",
    "\n",
    "- **Sequence Classification**: sentences/texts as sequences of linguistic units (words)\n",
    "\n",
    "\n",
    "- Annotation on different levels of linguistic description:\n",
    "  -  ** SEGMENTATION &rarr; MORPHOLOGICAL &rarr; SYNTACTIC &rarr; SEMANTIC ANALYSIS **\n",
    "- NLP Pipeline: models for different subsequent annotation tasks \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.nltk.org/images/tag-context.png\" width=44%>\n",
    "https://www.nltk.org/book/ch05.html#fig-tag-context\n",
    "> When we perform a language processing task based on unigrams, we are using one item of **context**. In the case of tagging, we only consider the current token, in isolation from any larger context. Given such a model, the best we can do is tag each word with its a priori most likely tag. This means we would tag a word such as *wind* with the same tag, regardless of whether it appears in the context *the wind* or *to wind*.\n",
    "\n",
    "> An n-gram tagger is a generalization of a unigram tagger whose context is the current word together with the part-of-speech tags of the n-1 preceding tokens, as shown in 5.1. The tag to be chosen, tn, is circled, and the context is shaded in grey. In the example of an n-gram tagger shown in 5.1, we have n=3; that is, we consider the tags of the two preceding words in addition to the current word. An n-gram tagger picks the tag that is most likely in the given context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Rule-based Models vs Machine Learning NLP Models\n",
    "\n",
    "### Rule-based Models\n",
    "- set of rules specifying a model \n",
    "- examples:\n",
    "  - rule-based tokenizer (see https://stanfordnlp.github.io/stanza/tokenize.html#use-spacy-for-fast-tokenization-and-sentence-segmentation)\n",
    "  - morphological regular expression tagger with NLTK: https://www.nltk.org/book/ch05.html#the-regular-expression-tagger\n",
    "\n",
    "\n",
    "### Machine Learning Models\n",
    "- learn rules (mapping from input to output) from training data\n",
    "- models based on patterns in sample data (training data)\n",
    "\n",
    "\n",
    "- different types of ML algorithms:\n",
    "    - statistical models \n",
    "    - neural network models (deep learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Training NLP Models: Feature Extraction + ML Algorithm\n",
    "\n",
    "- **learn mapping: *feature-representation of linguistic unit*  &rarr; *class label* **\n",
    "- **based on labeled training data**\n",
    "\n",
    "\n",
    "- training data for NLP models: annotated corpora (e.g. POS-annotated for POS tagger model)\n",
    "\n",
    "\n",
    "- result: Classification Model (Classifier) for predicting annotation labels (e.g. POS-tags)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.nltk.org/images/supervised-classification.png\" width=44%>\n",
    "https://www.nltk.org/book/ch06.html#fig-supervised-classification\n",
    ">Supervised Classification. <br>(a) During training, a feature extractor is used to convert each input value to a feature set [...] which capture the basic information about each input that should be used to classify it [...]. Pairs of feature sets and labels are fed into the machine learning algorithm to generate a model. <br>(b) During prediction, the same feature extractor is used to convert unseen inputs to feature sets. These feature sets are then fed into the model, which generates predicted labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## 5. Main Components of NLP Pipeline\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "> To start annotating text with Stanza, you would typically start by building a `Pipeline` that contains Processors, each fulfilling a specific NLP task you desire (e.g., tokenization, part-of-speech tagging, syntactic parsing, etc). The pipeline takes in raw text or a `Document` object that contains partial annotations, runs the specified processors in succession, and returns an annotated `Document` (see the documentation on `Document` for more information on how to extract these annotations).<br>https://stanfordnlp.github.io/stanza/pipeline.html\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "<img src=\"https://stanfordnlp.github.io/stanza/assets/images/pipeline.png\" width=74%>\n",
    "*Stanza Pipeline Overview, https://stanfordnlp.github.io/stanza/*\n",
    "\n",
    "\n",
    "---\n",
    "### Segmentation / Tokenization\n",
    "\n",
    "- split string into linguistic units (see https://www.nltk.org/book/ch03.html#sec-tokenization)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "### Lemmatization / Stemming\n",
    "\n",
    ">- stemming: strip off any affixes\n",
    "- lemmatization: resulting form as known word in a dictionary<br>https://www.nltk.org/book/ch03.html#sec-normalizing-text\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "###  POS tagging & morphological feature\n",
    "\n",
    "- assign Part-of-Speech-categories to words \n",
    "- example Tagsets: \n",
    "  - STTS for german texts: https://homepage.ruhr-uni-bochum.de/stephen.berman/Korpuslinguistik/Tagsets-STTS.html\n",
    "  - Universal Tagset: https://universaldependencies.org/u/pos/\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "###  Dependency parsing\n",
    "\n",
    "- identify syntactical relations between words (hierarchical sentence structure)\n",
    "- builds on POS-analysis\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Downstream Tasks\n",
    "\n",
    "\n",
    "### Named Entitiy Recognition (NER)\n",
    "\n",
    "- extract & classify named entities from text (persons, locations, organizations)\n",
    "- mapping:  **token/phrase &rarr; entity class**\n",
    "\n",
    "\n",
    "### Sentiment Analysis\n",
    "\n",
    "- classify text according to subjective attitude \n",
    "- mapping:  **text &rarr; sentiment label**\n",
    "\n",
    "\n",
    "\n",
    "### (Semantic Parsing, e.g. for Question-Answering-Systems)\n",
    "\n",
    "- obtain logical representations for sentences (sentence meaning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Example 1: Pipeline for Information Extraction System\n",
    "<img src=\"https://www.nltk.org/images/ie-architecture.png\" width=\"74%\">\n",
    "https://www.nltk.org/book/ch01.html#fig-sds:\n",
    "> Simple Pipeline Architecture for an Information Extraction System. This system takes the raw text of a document as its input, and generates a list of (entity, relation, entity) tuples as its output. For example, given a document that indicates that the company Georgia-Pacific is located in Atlanta, it might generate the tuple `([ORG: 'Georgia-Pacific'] 'in' [LOC: 'Atlanta'])`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Example 2: Pipeline for Spoken Dialogue System\n",
    "<img src=\"https://www.nltk.org/images/dialogue.png\" width=\"44%\">\n",
    "https://www.nltk.org/book/ch07.html#fig-ie-architecture:\n",
    "> Simple Pipeline Architecture for a Spoken Dialogue System: Spoken input (top left) is analyzed, words are recognized, sentences are parsed and interpreted in context, application-specific actions take place (top right); a response is planned, realized as a syntactic structure, then to suitably inflected words, and finally to spoken output; different types of linguistic knowledge inform each stage of the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part II: Using *stanza* for Text Annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ---\n",
    "  \n",
    "## stanza (Stanford Python NLP Tool)\n",
    "\n",
    "#### Python NLP package with models for many languages  (*NLP = Natural Language Processing*)\n",
    "\n",
    "> Stanza is a collection of accurate and efficient tools for the linguistic analysis of many human languages. Starting from raw text to syntactic analysis and entity recognition, Stanza brings state-of-the-art NLP models to languages of your choosing.<br>\n",
    " https://stanfordnlp.github.io/stanza/\n",
    " \n",
    "- provides linguistic annotation layers for raw text input \n",
    "- support for many languages (see https://stanfordnlp.github.io/stanza/available_models.html)\n",
    "- machine learning NLP models, built with neural network components\n",
    "- models trained on large corpora (based on [UD-formalism](https://universaldependencies.org/)) \n",
    " \n",
    "\n",
    "- native Python package, alternative to well-known **Stanford CoreNLP Tools** (java-based):\n",
    "    - Stanford CoreNLP Online Demo: https://corenlp.run/\n",
    "    - Stanford Parer Online: http://nlp.stanford.edu:8080/parser/\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Loading Package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##  0.1 Downloading models for different languages:\n",
    "\n",
    "https://stanfordnlp.github.io/stanza/installation_usage.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Ancient Languages Models\n",
    "\n",
    "- trained on PROIEL corpora\n",
    "- https://stanfordnlp.github.io/stanza/available_models.html\n",
    "\n",
    "\n",
    "\n",
    "####  Ancient Languages Corpora\n",
    "- Example: PROIEL Treebank: https://github.com/proiel \n",
    "  - \"A dependency treebank for ancient Indo-European languages\"\n",
    "  - Part of Syntacticus: http://dev.syntacticus.org/ (umbrella project for corpora of early IE languages)\n",
    "  \n",
    "  - Example: Latin PROIEL Corpus:\n",
    "    - Tabular conll-Format with dependency relations\n",
    "    - Download: https://raw.githubusercontent.com/proiel/proiel-treebank/master/latin-nt.conll\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "### Downloading Ancient Greek Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "197af6bcd4e8492dab2a49356d2765c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-29 10:42:33 INFO: Downloading default packages for language: grc (Ancient_Greek)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "302fe98b98854af28959fac022525a88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-29 10:43:00 INFO: Finished downloading models and saved to /stanza_resources.\n"
     ]
    }
   ],
   "source": [
    "#stanza.download('grc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### (Inspect stanza's Neural Network Model for Ancient-Greek-Tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##inspect neural model (tokenize.pt for grc = ancient greek)\n",
    "#import torch\n",
    "#from os.path import expanduser\n",
    "#home = expanduser(\"~\")\n",
    "#torch.load(home+'/stanza_resources/grc/tokenize/proiel.pt',map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Downloading Latin Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c2bdc7c80a44ca79666199e1b485720",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-29 10:43:25 INFO: Downloading default packages for language: la (Latin)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "045cb48adac6406babe0981afe524774",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-29 10:44:10 INFO: Finished downloading models and saved to /stanza_resources.\n"
     ]
    }
   ],
   "source": [
    "#stanza.download('la')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Downloading English & German Models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stanza.download('en')\n",
    "#stanza.download('de')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 0.2 Initialize `NLP`-Pipeline for Language (Load Models)\n",
    "\n",
    "https://stanfordnlp.github.io/stanza/pipeline.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-21 07:29:41 INFO: Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | combined  |\n",
      "| pos       | combined  |\n",
      "| lemma     | combined  |\n",
      "| depparse  | combined  |\n",
      "| sentiment | sstplus   |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "2022-01-21 07:29:41 INFO: Use device: cpu\n",
      "2022-01-21 07:29:41 INFO: Loading: tokenize\n",
      "2022-01-21 07:29:41 INFO: Loading: pos\n",
      "2022-01-21 07:29:41 INFO: Loading: lemma\n",
      "2022-01-21 07:29:41 INFO: Loading: depparse\n",
      "2022-01-21 07:29:42 INFO: Loading: sentiment\n",
      "2022-01-21 07:29:42 INFO: Loading: ner\n",
      "2022-01-21 07:29:43 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(lang='en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.3 Run Annotation on Sentence/Text\n",
    "\n",
    "### Creates `Document` Object\n",
    "\n",
    ">  Document object holds the annotation of an entire document, and is automatically generated when a string is annotated by the Pipeline. It contains a collection of Sentences and entities (which are represented as Spans), and can be seamlessly translated into a native Python object.<br>https://stanfordnlp.github.io/stanza/data_objects.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\n",
       "  [\n",
       "    {\n",
       "      \"id\": 1,\n",
       "      \"text\": \"To\",\n",
       "      \"lemma\": \"to\",\n",
       "      \"upos\": \"PART\",\n",
       "      \"xpos\": \"TO\",\n",
       "      \"head\": 2,\n",
       "      \"deprel\": \"mark\",\n",
       "      \"start_char\": 0,\n",
       "      \"end_char\": 2,\n",
       "      \"ner\": \"O\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": 2,\n",
       "      \"text\": \"be\",\n",
       "      \"lemma\": \"be\",\n",
       "      \"upos\": \"VERB\",\n",
       "      \"xpos\": \"VB\",\n",
       "      \"feats\": \"VerbForm=Inf\",\n",
       "      \"head\": 0,\n",
       "      \"deprel\": \"root\",\n",
       "      \"start_char\": 3,\n",
       "      \"end_char\": 5,\n",
       "      \"ner\": \"O\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": 3,\n",
       "      \"text\": \"or\",\n",
       "      \"lemma\": \"or\",\n",
       "      \"upos\": \"CCONJ\",\n",
       "      \"xpos\": \"CC\",\n",
       "      \"head\": 4,\n",
       "      \"deprel\": \"cc\",\n",
       "      \"start_char\": 6,\n",
       "      \"end_char\": 8,\n",
       "      \"ner\": \"O\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": 4,\n",
       "      \"text\": \"not\",\n",
       "      \"lemma\": \"not\",\n",
       "      \"upos\": \"PART\",\n",
       "      \"xpos\": \"RB\",\n",
       "      \"head\": 2,\n",
       "      \"deprel\": \"conj\",\n",
       "      \"start_char\": 9,\n",
       "      \"end_char\": 12,\n",
       "      \"ner\": \"O\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": 5,\n",
       "      \"text\": \"to\",\n",
       "      \"lemma\": \"to\",\n",
       "      \"upos\": \"PART\",\n",
       "      \"xpos\": \"TO\",\n",
       "      \"head\": 6,\n",
       "      \"deprel\": \"mark\",\n",
       "      \"start_char\": 13,\n",
       "      \"end_char\": 15,\n",
       "      \"ner\": \"O\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": 6,\n",
       "      \"text\": \"be\",\n",
       "      \"lemma\": \"be\",\n",
       "      \"upos\": \"VERB\",\n",
       "      \"xpos\": \"VB\",\n",
       "      \"feats\": \"VerbForm=Inf\",\n",
       "      \"head\": 2,\n",
       "      \"deprel\": \"xcomp\",\n",
       "      \"start_char\": 16,\n",
       "      \"end_char\": 18,\n",
       "      \"ner\": \"O\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": 7,\n",
       "      \"text\": \".\",\n",
       "      \"lemma\": \".\",\n",
       "      \"upos\": \"PUNCT\",\n",
       "      \"xpos\": \".\",\n",
       "      \"head\": 2,\n",
       "      \"deprel\": \"punct\",\n",
       "      \"start_char\": 18,\n",
       "      \"end_char\": 19,\n",
       "      \"ner\": \"O\"\n",
       "    }\n",
       "  ]\n",
       "]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp('To be or not to be.')\n",
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.4 Output Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: To \tlemma: to\tpos: PART\n",
      "word: be \tlemma: be\tpos: VERB\n",
      "word: or \tlemma: or\tpos: CCONJ\n",
      "word: not \tlemma: not\tpos: PART\n",
      "word: to \tlemma: to\tpos: PART\n",
      "word: be \tlemma: be\tpos: VERB\n",
      "word: . \tlemma: .\tpos: PUNCT\n"
     ]
    }
   ],
   "source": [
    "print(*[f'word: {word.text+\" \"}\\tlemma: {word.lemma}\\tpos: {word.pos}' for sent in doc.sentences for word in sent.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.5 Optional: Specifiy Processors (Annotation layers) = Building Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-21 07:30:28 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "| pos       | combined |\n",
      "| lemma     | combined |\n",
      "========================\n",
      "\n",
      "2022-01-21 07:30:28 INFO: Use device: cpu\n",
      "2022-01-21 07:30:28 INFO: Loading: tokenize\n",
      "2022-01-21 07:30:28 INFO: Loading: pos\n",
      "2022-01-21 07:30:28 INFO: Loading: lemma\n",
      "2022-01-21 07:30:28 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,pos,lemma') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention: Pipeline Requirements must be met!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-21 07:30:31 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| lemma     | combined |\n",
      "========================\n",
      "\n",
      "2022-01-21 07:30:31 INFO: Use device: cpu\n",
      "2022-01-21 07:30:31 INFO: Loading: lemma\n",
      "2022-01-21 07:30:31 INFO: \n",
      "\n"
     ]
    },
    {
     "ename": "PipelineRequirementsException",
     "evalue": "\n\n---\nPipeline Requirements Error!\n\tProcessor: LemmaProcessor\n\tPipeline processors list: lemma\n\tProcessor Requirements: {'tokenize'}\n\t\t- fulfilled: set()\n\t\t- missing: {'tokenize'}\n\nThe processors list provided for this pipeline is invalid.  Please make sure all prerequisites are met for every processor.\n\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPipelineRequirementsException\u001b[0m             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-d0a0335f19f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstanza\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'en'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'lemma'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Library/Python/3.6/lib/python/site-packages/stanza/pipeline/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, lang, dir, package, processors, logging_level, verbose, use_gpu, model_dir, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpipeline_reqs_exceptions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mPipelineRequirementsException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline_reqs_exceptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Done loading processors!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPipelineRequirementsException\u001b[0m: \n\n---\nPipeline Requirements Error!\n\tProcessor: LemmaProcessor\n\tPipeline processors list: lemma\n\tProcessor Requirements: {'tokenize'}\n\t\t- fulfilled: set()\n\t\t- missing: {'tokenize'}\n\nThe processors list provided for this pipeline is invalid.  Please make sure all prerequisites are met for every processor.\n\n\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(lang='en', processors='lemma') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## 1. Tokenization\n",
    "\n",
    "- Sentence & Word Tokenization \n",
    "- text as list of lists of words\n",
    "\n",
    "> Tokenization and sentence segmentation in Stanza are jointly performed by the TokenizeProcessor. This processor splits the raw input text into tokens and sentences, so that downstream annotation can happen at the sentence level.<br>https://stanfordnlp.github.io/stanza/tokenize.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-21 07:30:40 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "========================\n",
      "\n",
      "2022-01-21 07:30:40 INFO: Use device: cpu\n",
      "2022-01-21 07:30:40 INFO: Loading: tokenize\n",
      "2022-01-21 07:30:40 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: He \n",
      "word: does \n",
      "word: n't \n",
      "word: stop \n",
      "word: . \n"
     ]
    }
   ],
   "source": [
    "# initialize English neural pipeline\n",
    "nlp = stanza.Pipeline(lang='en', processors='tokenize')\n",
    "\n",
    "# run annotation:\n",
    "doc = nlp(\"\"\"He doesn't stop. \"\"\")\n",
    "\n",
    "# output:\n",
    "print(*[f'word: {word.text+\" \"}' for sent in doc.sentences for word in sent.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Multi-Word Token (MWT) Expansion (only for German, Spanish, French)\n",
    "- https://stanfordnlp.github.io/stanza/mwt.html\n",
    "- https://stanfordnlp.github.io/CoreNLP/mwt.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-21 07:30:40 INFO: Loading these models for language: de (German):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | gsd     |\n",
      "| mwt       | gsd     |\n",
      "=======================\n",
      "\n",
      "2022-01-21 07:30:40 INFO: Use device: cpu\n",
      "2022-01-21 07:30:40 INFO: Loading: tokenize\n",
      "2022-01-21 07:30:40 INFO: Loading: mwt\n",
      "2022-01-21 07:30:40 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: Ich \n",
      "word: sage \n",
      "word: es \n",
      "word: zu \n",
      "word: dem \n",
      "word: letzen \n",
      "word: Mal \n",
      "word: . \n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(lang='de', processors='tokenize, mwt')\n",
    "doc = nlp(\"\"\"Ich sage es zum letzen Mal.\"\"\")\n",
    "\n",
    "print(*[f'word: {word.text+\" \"}' for sent in doc.sentences for word in sent.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Tokenization without Sentence Segmentation\n",
    "\n",
    "> Sometimes you might want to tokenize your text given existing sentences (e.g., in machine translation). You can perform tokenization without sentence segmentation, as long as the sentences are split by two continuous newlines (\\n\\n) in the raw text.<br>https://stanfordnlp.github.io/stanza/tokenize.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-21 07:30:40 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "========================\n",
      "\n",
      "2022-01-21 07:30:40 INFO: Use device: cpu\n",
      "2022-01-21 07:30:40 INFO: Loading: tokenize\n",
      "2022-01-21 07:30:40 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Sentence 1 tokens =======\n",
      "id: (1,)\ttext: This\n",
      "id: (2,)\ttext: is\n",
      "id: (3,)\ttext: a\n",
      "id: (4,)\ttext: sentence\n",
      "id: (5,)\ttext: .\n",
      "====== Sentence 2 tokens =======\n",
      "id: (1,)\ttext: This\n",
      "id: (2,)\ttext: is\n",
      "id: (3,)\ttext: a\n",
      "id: (4,)\ttext: second\n",
      "id: (5,)\ttext: .\n",
      "id: (6,)\ttext: This\n",
      "id: (7,)\ttext: is\n",
      "id: (8,)\ttext: a\n",
      "id: (9,)\ttext: third\n",
      "id: (10,)\ttext: .\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(lang='en', processors='tokenize', tokenize_no_ssplit=True)\n",
    "doc = nlp('This is a sentence.\\n\\nThis is a second. This is a third.')\n",
    "\n",
    "for i, sentence in enumerate(doc.sentences):\n",
    "    print(f'====== Sentence {i+1} tokens =======')\n",
    "    print(*[f'id: {token.id}\\ttext: {token.text}' for token in sentence.tokens], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1.3 Start with Pretokenized Text\n",
    "\n",
    "> In some cases, you might have already tokenized your text, and just want to use Stanza for downstream processing. In these cases, you can feed in pretokenized (and sentence split) text to the pipeline, as newline (\\n) separated sentences, where each sentence is space separated tokens.<br>https://stanfordnlp.github.io/stanza/tokenize.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-21 07:30:40 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "========================\n",
      "\n",
      "2022-01-21 07:30:40 INFO: Use device: cpu\n",
      "2022-01-21 07:30:40 INFO: Loading: tokenize\n",
      "2022-01-21 07:30:40 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Sentence 1 tokens =======\n",
      "id: (1,)\ttext: This\n",
      "id: (2,)\ttext: is\n",
      "id: (3,)\ttext: token.ization\n",
      "id: (4,)\ttext: done\n",
      "id: (5,)\ttext: my\n",
      "id: (6,)\ttext: way!\n",
      "====== Sentence 2 tokens =======\n",
      "id: (1,)\ttext: Sentence\n",
      "id: (2,)\ttext: split,\n",
      "id: (3,)\ttext: too!\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(lang='en', processors='tokenize', tokenize_pretokenized=True)\n",
    "doc = nlp('This is token.ization done my way!\\nSentence split, too!')\n",
    "\n",
    "for i, sentence in enumerate(doc.sentences):\n",
    "    print(f'====== Sentence {i+1} tokens =======')\n",
    "    print(*[f'id: {token.id}\\ttext: {token.text}' for token in sentence.tokens], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Lemmatization\n",
    "\n",
    "https://stanfordnlp.github.io/stanza/lemma.html\n",
    "\n",
    "### *Requirements:* `tokenize, mwt, pos`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-21 07:30:40 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "| lemma     | combined |\n",
      "========================\n",
      "\n",
      "2022-01-21 07:30:40 INFO: Use device: cpu\n",
      "2022-01-21 07:30:40 INFO: Loading: tokenize\n",
      "2022-01-21 07:30:40 INFO: Loading: lemma\n",
      "2022-01-21 07:30:40 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: Call \tlemma: call\n",
      "word: me \tlemma: I\n",
      "word: Ishmael \tlemma: ishmael\n",
      "word: . \tlemma: .\n",
      "word: Some \tlemma: some\n",
      "word: years \tlemma: year\n",
      "word: ago \tlemma: ago\n",
      "word: — \tlemma: —\n",
      "word: never \tlemma: never\n",
      "word: mind \tlemma: mind\n",
      "word: how \tlemma: how\n",
      "word: long \tlemma: long\n",
      "word: precisely \tlemma: precisely\n",
      "word: — \tlemma: —\n",
      "word: having \tlemma: have\n",
      "word: little \tlemma: little\n",
      "word: or \tlemma: or\n",
      "word: no \tlemma: no\n",
      "word: money \tlemma: money\n",
      "word: in \tlemma: in\n",
      "word: my \tlemma: my\n",
      "word: purse \tlemma: purse\n",
      "word: , \tlemma: ,\n",
      "word: and \tlemma: and\n",
      "word: nothing \tlemma: nothing\n",
      "word: particular \tlemma: particular\n",
      "word: to \tlemma: to\n",
      "word: interest \tlemma: interest\n",
      "word: me \tlemma: I\n",
      "word: on \tlemma: on\n",
      "word: shore \tlemma: shore\n",
      "word: , \tlemma: ,\n",
      "word: I \tlemma: I\n",
      "word: thought \tlemma: think\n",
      "word: I \tlemma: I\n",
      "word: would \tlemma: would\n",
      "word: sail \tlemma: sail\n",
      "word: about \tlemma: about\n",
      "word: a \tlemma: a\n",
      "word: little \tlemma: little\n",
      "word: and \tlemma: and\n",
      "word: see \tlemma: see\n",
      "word: the \tlemma: the\n",
      "word: watery \tlemma: watery\n",
      "word: part \tlemma: part\n",
      "word: of \tlemma: of\n",
      "word: the \tlemma: the\n",
      "word: world \tlemma: world\n",
      "word: . \tlemma: .\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(lang='en', processors='tokenize, lemma') \n",
    "\n",
    "doc = nlp(\"\"\"Call me Ishmael. Some years ago—never mind how long precisely—having little or no money in my purse, and nothing particular to interest me on shore, I thought I would sail about a little and see the watery part of the world.\"\"\")\n",
    "\n",
    "print(*[f'word: {word.text+\" \"}\\tlemma: {word.lemma}' for sent in doc.sentences for word in sent.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##  3. Part-of-Speech & Morphological Features\n",
    "\n",
    "> The Part-of-Speech (POS) & morphological features tagging module labels words with their universal POS (UPOS) tags, treebank-specific POS (XPOS) tags, and universal morphological features (UFeats).<br>https://stanfordnlp.github.io/stanza/pos.html\n",
    "\n",
    "### *Requirements:* `tokenize, mwt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-21 07:30:40 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "| pos       | combined |\n",
      "========================\n",
      "\n",
      "2022-01-21 07:30:40 INFO: Use device: cpu\n",
      "2022-01-21 07:30:40 INFO: Loading: tokenize\n",
      "2022-01-21 07:30:40 INFO: Loading: pos\n",
      "2022-01-21 07:30:41 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: Call\tupos: VERB\txpos: VB\tfeats: Mood=Imp|VerbForm=Fin\n",
      "word: me\tupos: PRON\txpos: PRP\tfeats: Case=Acc|Number=Sing|Person=1|PronType=Prs\n",
      "word: Ishmael\tupos: PROPN\txpos: NNP\tfeats: Number=Sing\n",
      "word: .\tupos: PUNCT\txpos: .\tfeats: _\n",
      "word: Some\tupos: DET\txpos: DT\tfeats: _\n",
      "word: years\tupos: NOUN\txpos: NNS\tfeats: Number=Plur\n",
      "word: ago\tupos: ADV\txpos: RB\tfeats: Degree=Pos\n",
      "word: —\tupos: PUNCT\txpos: :\tfeats: _\n",
      "word: never\tupos: ADV\txpos: RB\tfeats: _\n",
      "word: mind\tupos: VERB\txpos: VB\tfeats: Mood=Imp|Person=2|VerbForm=Fin\n",
      "word: how\tupos: SCONJ\txpos: WRB\tfeats: PronType=Int\n",
      "word: long\tupos: ADV\txpos: RB\tfeats: Degree=Pos\n",
      "word: precisely\tupos: ADV\txpos: RB\tfeats: Degree=Pos\n",
      "word: —\tupos: PUNCT\txpos: :\tfeats: _\n",
      "word: having\tupos: VERB\txpos: VBG\tfeats: VerbForm=Ger\n",
      "word: little\tupos: ADJ\txpos: JJ\tfeats: Degree=Pos\n",
      "word: or\tupos: CCONJ\txpos: CC\tfeats: _\n",
      "word: no\tupos: DET\txpos: DT\tfeats: Polarity=Neg\n",
      "word: money\tupos: NOUN\txpos: NN\tfeats: Number=Sing\n",
      "word: in\tupos: ADP\txpos: IN\tfeats: _\n",
      "word: my\tupos: PRON\txpos: PRP$\tfeats: Number=Sing|Person=1|Poss=Yes|PronType=Prs\n",
      "word: purse\tupos: NOUN\txpos: NN\tfeats: Number=Sing\n",
      "word: ,\tupos: PUNCT\txpos: ,\tfeats: _\n",
      "word: and\tupos: CCONJ\txpos: CC\tfeats: _\n",
      "word: nothing\tupos: PRON\txpos: NN\tfeats: Number=Sing\n",
      "word: particular\tupos: ADJ\txpos: JJ\tfeats: Degree=Pos\n",
      "word: to\tupos: PART\txpos: TO\tfeats: _\n",
      "word: interest\tupos: VERB\txpos: VB\tfeats: VerbForm=Inf\n",
      "word: me\tupos: PRON\txpos: PRP\tfeats: Case=Acc|Number=Sing|Person=1|PronType=Prs\n",
      "word: on\tupos: ADP\txpos: IN\tfeats: _\n",
      "word: shore\tupos: NOUN\txpos: NN\tfeats: Number=Sing\n",
      "word: ,\tupos: PUNCT\txpos: ,\tfeats: _\n",
      "word: I\tupos: PRON\txpos: PRP\tfeats: Case=Nom|Number=Sing|Person=1|PronType=Prs\n",
      "word: thought\tupos: VERB\txpos: VBD\tfeats: Mood=Ind|Number=Sing|Person=1|Tense=Past|VerbForm=Fin\n",
      "word: I\tupos: PRON\txpos: PRP\tfeats: Case=Nom|Number=Sing|Person=1|PronType=Prs\n",
      "word: would\tupos: AUX\txpos: MD\tfeats: VerbForm=Fin\n",
      "word: sail\tupos: VERB\txpos: VB\tfeats: VerbForm=Inf\n",
      "word: about\tupos: ADP\txpos: IN\tfeats: _\n",
      "word: a\tupos: DET\txpos: DT\tfeats: Definite=Ind|PronType=Art\n",
      "word: little\tupos: ADJ\txpos: JJ\tfeats: Degree=Pos\n",
      "word: and\tupos: CCONJ\txpos: CC\tfeats: _\n",
      "word: see\tupos: VERB\txpos: VB\tfeats: VerbForm=Inf\n",
      "word: the\tupos: DET\txpos: DT\tfeats: Definite=Def|PronType=Art\n",
      "word: watery\tupos: ADJ\txpos: JJ\tfeats: Degree=Pos\n",
      "word: part\tupos: NOUN\txpos: NN\tfeats: Number=Sing\n",
      "word: of\tupos: ADP\txpos: IN\tfeats: _\n",
      "word: the\tupos: DET\txpos: DT\tfeats: Definite=Def|PronType=Art\n",
      "word: world\tupos: NOUN\txpos: NN\tfeats: Number=Sing\n",
      "word: .\tupos: PUNCT\txpos: .\tfeats: _\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,pos')\n",
    "doc = nlp(\"\"\"Call me Ishmael. Some years ago—never mind how long precisely—having little or no money in my purse, and nothing particular to interest me on shore, I thought I would sail about a little and see the watery part of the world.\"\"\")\n",
    "\n",
    "print(*[f'word: {word.text}\\tupos: {word.upos}\\txpos: {word.xpos}\\tfeats: {word.feats if word.feats else \"_\"}' for sent in doc.sentences for word in sent.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Dependency Parsing\n",
    "\n",
    "> The dependency parsing module builds a tree structure of words from the input sentence, which represents the syntactic dependency relations between words. The resulting tree representations, which follow the Universal Dependencies formalism, are useful in many downstream applications.<br>https://stanfordnlp.github.io/stanza/depparse.html\n",
    "\n",
    "\n",
    "### *Requirements:* `tokenize, mwt, pos, lemma`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-21 07:30:41 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "| pos       | combined |\n",
      "| lemma     | combined |\n",
      "| depparse  | combined |\n",
      "========================\n",
      "\n",
      "2022-01-21 07:30:41 INFO: Use device: cpu\n",
      "2022-01-21 07:30:41 INFO: Loading: tokenize\n",
      "2022-01-21 07:30:41 INFO: Loading: pos\n",
      "2022-01-21 07:30:41 INFO: Loading: lemma\n",
      "2022-01-21 07:30:41 INFO: Loading: depparse\n",
      "2022-01-21 07:30:42 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 1\tword: Chris\thead id: 3\thead: teaches\tdeprel: nsubj\n",
      "id: 2\tword: Manning\thead id: 1\thead: Chris\tdeprel: flat\n",
      "id: 3\tword: teaches\thead id: 0\thead: root\tdeprel: root\n",
      "id: 4\tword: at\thead id: 6\thead: University\tdeprel: case\n",
      "id: 5\tword: Stanford\thead id: 6\thead: University\tdeprel: compound\n",
      "id: 6\tword: University\thead id: 3\thead: teaches\tdeprel: obl\n",
      "id: 7\tword: .\thead id: 3\thead: teaches\tdeprel: punct\n",
      "id: 1\tword: He\thead id: 2\thead: lives\tdeprel: nsubj\n",
      "id: 2\tword: lives\thead id: 0\thead: root\tdeprel: root\n",
      "id: 3\tword: in\thead id: 6\thead: Area\tdeprel: case\n",
      "id: 4\tword: the\thead id: 6\thead: Area\tdeprel: det\n",
      "id: 5\tword: Bay\thead id: 6\thead: Area\tdeprel: compound\n",
      "id: 6\tword: Area\thead id: 2\thead: lives\tdeprel: obl\n",
      "id: 7\tword: .\thead id: 2\thead: lives\tdeprel: punct\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,pos,lemma,depparse')\n",
    "doc = nlp('Chris Manning teaches at Stanford University. He lives in the Bay Area.')\n",
    "\n",
    "print(*[f'id: {word.id}\\tword: {word.text}\\thead id: {word.head}\\thead: {sent.words[word.head-1].text if word.head > 0 else \"root\"}\\tdeprel: {word.deprel}' for sent in doc.sentences for word in sent.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### POS, Lemmatization & Dependencies with Ancient Greek & Latin Models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-21 07:30:42 INFO: Loading these models for language: grc (Ancient_Greek):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | proiel  |\n",
      "| pos       | proiel  |\n",
      "| lemma     | proiel  |\n",
      "| depparse  | proiel  |\n",
      "=======================\n",
      "\n",
      "2022-01-21 07:30:42 INFO: Use device: cpu\n",
      "2022-01-21 07:30:42 INFO: Loading: tokenize\n",
      "2022-01-21 07:30:42 INFO: Loading: pos\n",
      "2022-01-21 07:30:42 INFO: Loading: lemma\n",
      "2022-01-21 07:30:42 INFO: Loading: depparse\n",
      "2022-01-21 07:30:43 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: Ἄνδρα \tlemma: ἀνήρ\tpos: NOUN\tdeprel: obj\n",
      "word: μοι \tlemma: ἐγώ\tpos: PRON\tdeprel: iobj\n",
      "word: ἔννεπε, \tlemma: ἐννέπω\tpos: VERB\tdeprel: root\n",
      "word: Μοῦσα, \tlemma: Μοῦσα,\tpos: PROPN\tdeprel: obj\n",
      "word: πολύτροπον, \tlemma: πολύτροπος\tpos: ADJ\tdeprel: appos\n",
      "word: ὃς \tlemma: ὅς\tpos: PRON\tdeprel: nsubj:pass\n",
      "word: μάλα \tlemma: μάλα\tpos: ADV\tdeprel: advmod\n",
      "word: πολλὰ \tlemma: πολύς\tpos: ADJ\tdeprel: advmod\n",
      "word: πλάγχθη, \tlemma: πλάγω\tpos: VERB\tdeprel: acl\n",
      "word: ἐπεὶ \tlemma: ἐπεί\tpos: SCONJ\tdeprel: mark\n",
      "word: Τροίης \tlemma: Τροία\tpos: PROPN\tdeprel: nmod\n",
      "word: ἱερὸν \tlemma: ἱερόν\tpos: NOUN\tdeprel: obj\n",
      "word: πτολίεθρον \tlemma: πτολίεθρον\tpos: NOUN\tdeprel: obj\n",
      "word: ἔπερσε· \tlemma: περστίθημι\tpos: VERB\tdeprel: advcl\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(lang='grc')\n",
    "#text: https://de.wikipedia.org/wiki/Odyssee\n",
    "doc = nlp(\"\"\"Ἄνδρα μοι ἔννεπε, Μοῦσα, πολύτροπον, ὃς μάλα πολλὰ\n",
    "    πλάγχθη, ἐπεὶ Τροίης ἱερὸν πτολίεθρον ἔπερσε·\n",
    "    \"\"\")\n",
    "print(*[f'word: {word.text+\" \"}\\tlemma: {word.lemma}\\tpos: {word.pos}\\tdeprel: {word.deprel}' for sent in doc.sentences for word in sent.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-21 07:30:43 INFO: Loading these models for language: la (Latin):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | ittb    |\n",
      "| pos       | ittb    |\n",
      "| lemma     | ittb    |\n",
      "| depparse  | ittb    |\n",
      "=======================\n",
      "\n",
      "2022-01-21 07:30:43 INFO: Use device: cpu\n",
      "2022-01-21 07:30:43 INFO: Loading: tokenize\n",
      "2022-01-21 07:30:43 INFO: Loading: pos\n",
      "2022-01-21 07:30:43 INFO: Loading: lemma\n",
      "2022-01-21 07:30:44 INFO: Loading: depparse\n",
      "2022-01-21 07:30:44 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: Et \tlemma: Et\tpos: CCONJ\tdeprel: cc\n",
      "word: iam \tlemma: iam\tpos: ADV\tdeprel: advmod:emph\n",
      "word: iamque \tlemma: iamque\tpos: ADV\tdeprel: advmod\n",
      "word: magis \tlemma: magis\tpos: ADV\tdeprel: advmod\n",
      "word: cunctantem \tlemma: cunctans\tpos: NOUN\tdeprel: obj\n",
      "word: flectere \tlemma: flecto\tpos: VERB\tdeprel: xcomp\n",
      "word: sermo \tlemma: sermo\tpos: NOUN\tdeprel: nsubj\n",
      "word: coeperat \tlemma: coepi\tpos: VERB\tdeprel: root\n",
      "word: , \tlemma: ,\tpos: PUNCT\tdeprel: punct\n",
      "word: infelix \tlemma: infelix\tpos: NOUN\tdeprel: conj\n",
      "word: umero \tlemma: umerum\tpos: NOUN\tdeprel: orphan\n",
      "word: cum \tlemma: cum\tpos: SCONJ\tdeprel: mark\n",
      "word: apparuit \tlemma: appareo\tpos: VERB\tdeprel: advcl\n",
      "word: alto \tlemma: altus\tpos: NOUN\tdeprel: obl\n",
      "word: balteus \tlemma: balteus\tpos: ADJ\tdeprel: nsubj\n",
      "word: et \tlemma: et\tpos: CCONJ\tdeprel: cc\n",
      "word: notis \tlemma: nosco\tpos: VERB\tdeprel: conj\n",
      "word: fulserunt \tlemma: sulso\tpos: VERB\tdeprel: conj\n",
      "word: cingula \tlemma: cingula\tpos: NOUN\tdeprel: nsubj\n",
      "word: bullis \tlemma: bullum\tpos: NOUN\tdeprel: obl\n",
      "word: Pallantis \tlemma: Pallantis\tpos: VERB\tdeprel: acl\n",
      "word: pueri \tlemma: puer\tpos: NOUN\tdeprel: nmod\n",
      "word: , \tlemma: ,\tpos: PUNCT\tdeprel: punct\n",
      "word: victum \tlemma: victum\tpos: NOUN\tdeprel: obj\n",
      "word: quem \tlemma: qui\tpos: PRON\tdeprel: obj\n",
      "word: vulnere \tlemma: vulnere\tpos: NOUN\tdeprel: obl\n",
      "word: Turnus \tlemma: Turnus\tpos: PROPN\tdeprel: nsubj\n",
      "word: straverat \tlemma: straverat\tpos: VERB\tdeprel: acl:relcl\n",
      "word: atque \tlemma: atque\tpos: CCONJ\tdeprel: cc\n",
      "word: umeris \tlemma: umerus\tpos: NOUN\tdeprel: obl:arg\n",
      "word: inimicum \tlemma: inimicus\tpos: ADJ\tdeprel: amod\n",
      "word: insigne \tlemma: insignus\tpos: ADJ\tdeprel: obj\n",
      "word: gerebat \tlemma: gero\tpos: VERB\tdeprel: conj\n",
      "word: . \tlemma: .\tpos: PUNCT\tdeprel: punct\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(lang='la')\n",
    "#text: https://de.wikipedia.org/wiki/Aeneis#Textbeispiel:_Das_Ende_der_Aeneis_(12,_940%E2%80%93952)\n",
    "doc = nlp(\"\"\"Et iam iamque magis cunctantem flectere sermo\n",
    "    coeperat, infelix umero cum apparuit alto\n",
    "    balteus et notis fulserunt cingula bullis\n",
    "    Pallantis pueri, victum quem vulnere Turnus\n",
    "    straverat atque umeris inimicum insigne gerebat.\n",
    "    \"\"\")\n",
    "print(*[f'word: {word.text+\" \"}\\tlemma: {word.lemma}\\tpos: {word.pos}\\tdeprel: {word.deprel}' for sent in doc.sentences for word in sent.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##  5. Sentiment Analysis\n",
    "\n",
    "https://stanfordnlp.github.io/stanza/sentiment.html\n",
    "\n",
    "### *Requirements:* `tokenize`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-21 07:30:44 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "| sentiment | sstplus  |\n",
      "========================\n",
      "\n",
      "2022-01-21 07:30:44 INFO: Use device: cpu\n",
      "2022-01-21 07:30:44 INFO: Loading: tokenize\n",
      "2022-01-21 07:30:44 INFO: Loading: sentiment\n",
      "2022-01-21 07:30:45 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence 0 : sentiment: 1\n",
      "sentence 1 : sentiment: 0\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,sentiment')\n",
    "doc = nlp('I like. I hate.')\n",
    "\n",
    "for i, sentence in enumerate(doc.sentences):\n",
    "    print('sentence',i, ': sentiment:', sentence.sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Named Entity Recognition (NER)\n",
    "\n",
    "> The named entity recognition (NER) module recognizes mention spans of a particular entity type (e.g., Person or Organization) in the input sentence. NER is widely used in many NLP applications such as information extraction or question answering systems.<br>https://stanfordnlp.github.io/stanza/ner.html\n",
    "\n",
    "### *Requirements:* `tokenize, mwt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-21 07:30:45 INFO: Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | combined  |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "2022-01-21 07:30:45 INFO: Use device: cpu\n",
      "2022-01-21 07:30:45 INFO: Loading: tokenize\n",
      "2022-01-21 07:30:45 INFO: Loading: ner\n",
      "2022-01-21 07:30:46 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entity: Chris Manning\ttype: PERSON\n",
      "entity: Stanford University\ttype: ORG\n",
      "entity: the Bay Area\ttype: LOC\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,ner')\n",
    "doc = nlp(\"Chris Manning teaches at Stanford University. He lives in the Bay Area.\")\n",
    "\n",
    "# sentence based NER output\n",
    "print(*[f'entity: {ent.text}\\ttype: {ent.type}' for ent in doc.ents], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token: Chris\tner: B-PERSON\n",
      "token: Manning\tner: E-PERSON\n",
      "token: teaches\tner: O\n",
      "token: at\tner: O\n",
      "token: Stanford\tner: B-ORG\n",
      "token: University\tner: E-ORG\n",
      "token: .\tner: O\n",
      "token: He\tner: O\n",
      "token: lives\tner: O\n",
      "token: in\tner: O\n",
      "token: the\tner: B-LOC\n",
      "token: Bay\tner: I-LOC\n",
      "token: Area\tner: E-LOC\n",
      "token: .\tner: O\n"
     ]
    }
   ],
   "source": [
    "# token based NER output\n",
    "print(*[f'token: {token.text}\\tner: {token.ner}' for sent in doc.sentences for token in sent.tokens], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{\n",
      "  \"text\": \"Chris Manning\",\n",
      "  \"type\": \"PERSON\",\n",
      "  \"start_char\": 0,\n",
      "  \"end_char\": 13\n",
      "}, {\n",
      "  \"text\": \"Stanford University\",\n",
      "  \"type\": \"ORG\",\n",
      "  \"start_char\": 25,\n",
      "  \"end_char\": 44\n",
      "}, {\n",
      "  \"text\": \"the Bay Area\",\n",
      "  \"type\": \"LOC\",\n",
      "  \"start_char\": 58,\n",
      "  \"end_char\": 70\n",
      "}]\n"
     ]
    }
   ],
   "source": [
    "print(doc.entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Named Entity Recognition und Visualization with spaCy\n",
    "\n",
    "#### spaCy: open-source Python NLP library with features comparable to stanza, intended for production usage\n",
    "\n",
    "- features visualizer for parse trees and NERs (displacy)\n",
    "\n",
    "https://spacy.io/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chris Manning PERSON\n",
      "Stanford University ORG\n",
      "the Bay Area LOC\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(\"Chris Manning teaches at Stanford University. He lives in the Bay Area.\")\n",
    "\n",
    "for word in doc.ents:\n",
    "    print(word.text,word.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"01085ec464d244189be9bede5e6a247d-0\" class=\"displacy\" width=\"1010\" height=\"257.0\" direction=\"ltr\" style=\"max-width: none; height: 257.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"167.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Chris</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"167.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"130\">Manning</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"130\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"167.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"210\">teaches</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"210\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"167.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"290\">at</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"290\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"167.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"370\">Stanford</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"370\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"167.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"450\">University.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"450\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"167.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"530\">He</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"530\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"167.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"610\">lives</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"610\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"167.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"690\">in</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"690\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"167.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"770\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"770\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"167.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"850\">Bay</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"850\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"167.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"930\">Area.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"930\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-01085ec464d244189be9bede5e6a247d-0-0\" stroke-width=\"2px\" d=\"M70,122.0 C70,82.0 120.0,82.0 120.0,122.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-01085ec464d244189be9bede5e6a247d-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,124.0 L62,112.0 78,112.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-01085ec464d244189be9bede5e6a247d-0-1\" stroke-width=\"2px\" d=\"M150,122.0 C150,82.0 200.0,82.0 200.0,122.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-01085ec464d244189be9bede5e6a247d-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M150,124.0 L142,112.0 158,112.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-01085ec464d244189be9bede5e6a247d-0-2\" stroke-width=\"2px\" d=\"M230,122.0 C230,82.0 280.0,82.0 280.0,122.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-01085ec464d244189be9bede5e6a247d-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M280.0,124.0 L288.0,112.0 272.0,112.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-01085ec464d244189be9bede5e6a247d-0-3\" stroke-width=\"2px\" d=\"M390,122.0 C390,82.0 440.0,82.0 440.0,122.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-01085ec464d244189be9bede5e6a247d-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M390,124.0 L382,112.0 398,112.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-01085ec464d244189be9bede5e6a247d-0-4\" stroke-width=\"2px\" d=\"M310,122.0 C310,42.0 445.0,42.0 445.0,122.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-01085ec464d244189be9bede5e6a247d-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M445.0,124.0 L453.0,112.0 437.0,112.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-01085ec464d244189be9bede5e6a247d-0-5\" stroke-width=\"2px\" d=\"M550,122.0 C550,82.0 600.0,82.0 600.0,122.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-01085ec464d244189be9bede5e6a247d-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M550,124.0 L542,112.0 558,112.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-01085ec464d244189be9bede5e6a247d-0-6\" stroke-width=\"2px\" d=\"M630,122.0 C630,82.0 680.0,82.0 680.0,122.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-01085ec464d244189be9bede5e6a247d-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M680.0,124.0 L688.0,112.0 672.0,112.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-01085ec464d244189be9bede5e6a247d-0-7\" stroke-width=\"2px\" d=\"M790,122.0 C790,42.0 925.0,42.0 925.0,122.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-01085ec464d244189be9bede5e6a247d-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M790,124.0 L782,112.0 798,112.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-01085ec464d244189be9bede5e6a247d-0-8\" stroke-width=\"2px\" d=\"M870,122.0 C870,82.0 920.0,82.0 920.0,122.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-01085ec464d244189be9bede5e6a247d-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M870,124.0 L862,112.0 878,112.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-01085ec464d244189be9bede5e6a247d-0-9\" stroke-width=\"2px\" d=\"M710,122.0 C710,2.0 930.0,2.0 930.0,122.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-01085ec464d244189be9bede5e6a247d-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M930.0,124.0 L938.0,112.0 922.0,112.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(doc, style=\"dep\", options={'distance':80})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Chris Manning\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " teaches at \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Stanford University\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ". He lives in \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the Bay Area\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Data Conversions\n",
    "\n",
    "https://stanfordnlp.github.io/stanza/data_conversion.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-30 10:55:46 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "| pos       | combined |\n",
      "| lemma     | combined |\n",
      "| depparse  | combined |\n",
      "========================\n",
      "\n",
      "2022-05-30 10:55:46 INFO: Use device: cpu\n",
      "2022-05-30 10:55:46 INFO: Loading: tokenize\n",
      "2022-05-30 10:55:46 INFO: Loading: pos\n",
      "2022-05-30 10:55:46 INFO: Loading: lemma\n",
      "2022-05-30 10:55:46 INFO: Loading: depparse\n",
      "2022-05-30 10:55:47 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,pos,lemma,depparse')\n",
    "doc = nlp('He lives in Munich.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stanza.models.common.doc.Document"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stanza.models.common.doc.Word"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc.sentences[0].words[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"id\": 1,\n",
       "  \"text\": \"He\",\n",
       "  \"lemma\": \"he\",\n",
       "  \"upos\": \"PRON\",\n",
       "  \"xpos\": \"PRP\",\n",
       "  \"feats\": \"Case=Nom|Gender=Masc|Number=Sing|Person=3|PronType=Prs\",\n",
       "  \"head\": 2,\n",
       "  \"deprel\": \"nsubj\",\n",
       "  \"start_char\": 0,\n",
       "  \"end_char\": 2\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.sentences[0].words[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He lives in Munich .\n"
     ]
    }
   ],
   "source": [
    "print(*[f'{word.text}' for sent in doc.sentences for word in sent.words], sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\n",
       "  [\n",
       "    {\n",
       "      \"id\": 1,\n",
       "      \"text\": \"He\",\n",
       "      \"lemma\": \"he\",\n",
       "      \"upos\": \"PRON\",\n",
       "      \"xpos\": \"PRP\",\n",
       "      \"feats\": \"Case=Nom|Gender=Masc|Number=Sing|Person=3|PronType=Prs\",\n",
       "      \"head\": 2,\n",
       "      \"deprel\": \"nsubj\",\n",
       "      \"start_char\": 0,\n",
       "      \"end_char\": 2\n",
       "    },\n",
       "    {\n",
       "      \"id\": 2,\n",
       "      \"text\": \"lives\",\n",
       "      \"lemma\": \"live\",\n",
       "      \"upos\": \"VERB\",\n",
       "      \"xpos\": \"VBZ\",\n",
       "      \"feats\": \"Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin\",\n",
       "      \"head\": 0,\n",
       "      \"deprel\": \"root\",\n",
       "      \"start_char\": 3,\n",
       "      \"end_char\": 8\n",
       "    },\n",
       "    {\n",
       "      \"id\": 3,\n",
       "      \"text\": \"in\",\n",
       "      \"lemma\": \"in\",\n",
       "      \"upos\": \"ADP\",\n",
       "      \"xpos\": \"IN\",\n",
       "      \"head\": 4,\n",
       "      \"deprel\": \"case\",\n",
       "      \"start_char\": 9,\n",
       "      \"end_char\": 11\n",
       "    },\n",
       "    {\n",
       "      \"id\": 4,\n",
       "      \"text\": \"Munich\",\n",
       "      \"lemma\": \"Munich\",\n",
       "      \"upos\": \"PROPN\",\n",
       "      \"xpos\": \"NNP\",\n",
       "      \"feats\": \"Number=Sing\",\n",
       "      \"head\": 2,\n",
       "      \"deprel\": \"obl\",\n",
       "      \"start_char\": 12,\n",
       "      \"end_char\": 18\n",
       "    },\n",
       "    {\n",
       "      \"id\": 5,\n",
       "      \"text\": \".\",\n",
       "      \"lemma\": \".\",\n",
       "      \"upos\": \"PUNCT\",\n",
       "      \"xpos\": \".\",\n",
       "      \"head\": 2,\n",
       "      \"deprel\": \"punct\",\n",
       "      \"start_char\": 18,\n",
       "      \"end_char\": 19\n",
       "    }\n",
       "  ]\n",
       "]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Object to Python Data Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dicts = doc.to_dict() # dicts is List[List[Dict]], representing each token / word in each sentence in the document\n",
    "type(dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He lives in Munich .\n"
     ]
    }
   ],
   "source": [
    "print(*[f'{word[\"text\"]}' for sent in dicts for word in sent], sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'deprel': 'nsubj',\n",
       " 'end_char': 2,\n",
       " 'feats': 'Case=Nom|Gender=Masc|Number=Sing|Person=3|PronType=Prs',\n",
       " 'head': 2,\n",
       " 'id': 1,\n",
       " 'lemma': 'he',\n",
       " 'start_char': 0,\n",
       " 'text': 'He',\n",
       " 'upos': 'PRON',\n",
       " 'xpos': 'PRP'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dicts[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'deprel': 'nsubj',\n",
       "   'end_char': 2,\n",
       "   'feats': 'Case=Nom|Gender=Masc|Number=Sing|Person=3|PronType=Prs',\n",
       "   'head': 2,\n",
       "   'id': 1,\n",
       "   'lemma': 'he',\n",
       "   'start_char': 0,\n",
       "   'text': 'He',\n",
       "   'upos': 'PRON',\n",
       "   'xpos': 'PRP'},\n",
       "  {'deprel': 'root',\n",
       "   'end_char': 8,\n",
       "   'feats': 'Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin',\n",
       "   'head': 0,\n",
       "   'id': 2,\n",
       "   'lemma': 'live',\n",
       "   'start_char': 3,\n",
       "   'text': 'lives',\n",
       "   'upos': 'VERB',\n",
       "   'xpos': 'VBZ'},\n",
       "  {'deprel': 'case',\n",
       "   'end_char': 11,\n",
       "   'head': 4,\n",
       "   'id': 3,\n",
       "   'lemma': 'in',\n",
       "   'start_char': 9,\n",
       "   'text': 'in',\n",
       "   'upos': 'ADP',\n",
       "   'xpos': 'IN'},\n",
       "  {'deprel': 'obl',\n",
       "   'end_char': 18,\n",
       "   'feats': 'Number=Sing',\n",
       "   'head': 2,\n",
       "   'id': 4,\n",
       "   'lemma': 'Munich',\n",
       "   'start_char': 12,\n",
       "   'text': 'Munich',\n",
       "   'upos': 'PROPN',\n",
       "   'xpos': 'NNP'},\n",
       "  {'deprel': 'punct',\n",
       "   'end_char': 19,\n",
       "   'head': 2,\n",
       "   'id': 5,\n",
       "   'lemma': '.',\n",
       "   'start_char': 18,\n",
       "   'text': '.',\n",
       "   'upos': 'PUNCT',\n",
       "   'xpos': '.'}]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Object to CoNLL (Tabular Dependency Parsing Format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stanza.utils.conll import CoNLL\n",
    "conll = CoNLL.convert_dict(dicts) # conll is List[List[List]], representing each token / word in each sentence in the document\n",
    "type(conll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['1',\n",
       "   'He',\n",
       "   'he',\n",
       "   'PRON',\n",
       "   'PRP',\n",
       "   'Case=Nom|Gender=Masc|Number=Sing|Person=3|PronType=Prs',\n",
       "   '2',\n",
       "   'nsubj',\n",
       "   '_',\n",
       "   'start_char=0|end_char=2'],\n",
       "  ['2',\n",
       "   'lives',\n",
       "   'live',\n",
       "   'VERB',\n",
       "   'VBZ',\n",
       "   'Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin',\n",
       "   '0',\n",
       "   'root',\n",
       "   '_',\n",
       "   'start_char=3|end_char=8'],\n",
       "  ['3',\n",
       "   'in',\n",
       "   'in',\n",
       "   'ADP',\n",
       "   'IN',\n",
       "   '_',\n",
       "   '4',\n",
       "   'case',\n",
       "   '_',\n",
       "   'start_char=9|end_char=11'],\n",
       "  ['4',\n",
       "   'Munich',\n",
       "   'Munich',\n",
       "   'PROPN',\n",
       "   'NNP',\n",
       "   'Number=Sing',\n",
       "   '2',\n",
       "   'obl',\n",
       "   '_',\n",
       "   'start_char=12|end_char=18'],\n",
       "  ['5',\n",
       "   '.',\n",
       "   '.',\n",
       "   'PUNCT',\n",
       "   '.',\n",
       "   '_',\n",
       "   '2',\n",
       "   'punct',\n",
       "   '_',\n",
       "   'start_char=18|end_char=19']]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# &Uuml;bungsaufgaben 3\n",
    "\n",
    "\n",
    "## Aufgabe 1 (eigenen Sentence-Segmenter erstellen)\n",
    "\n",
    "Satzsegmentierung (End-of-Sentence-Detection) kann als binäre Klassifikation verstanden werden (s. https://www.nltk.org/book/ch06.html#sentence-segmentation), die für jedes Token in einem Korpus entscheidet, ob es ein ***sentence boundary token*** ist oder nicht. Dies ist genauer eine **Sequenzklassifikation**, da die Entscheidung abhängt vom ***Kontext der Punktuationszeichen*** (z.B. `['Mr', '.']`).\n",
    "\n",
    "\n",
    "Erzeugen Sie einen ***(1) regelbasierten*** sowie einen ***(2) auf Satz-Segmentationsdaten der Penn-Treebank trainierten*** **Punktuationsklassifikator zur Satzsegmentierung**. \n",
    "\n",
    "Input soll eine Wordliste mit einer einfachen Tokenisierung sein, wie in folgendem englischen Beispielsatz, mit dem Sie Ihre Klassifikatoren auch testen sollen.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"You hear that Mr. Anderson? That is the sound of inevitability. Good-bye, Mr. Anderson! END\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['You', 'hear', 'that', 'Mr', '.', 'Anderson', '?', 'That', 'is', 'the', 'sound', 'of', 'inevitability', '.', 'Good', '-', 'bye', ',', 'Mr', '.', 'Anderson', '!', 'END']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "test_words = re.findall(r'\\w+|[^\\w\\s]+', text)  #entspricht nltk.wordpunct_tokenize\n",
    "print(test_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe 1a (Rule-based Sentence Segmentation)\n",
    "\n",
    "\n",
    "Erstellen Sie einen einfachen regelbasierten Punctuation Tagger, der eine Liste von Wort- und Punktuationstokens in eine Liste von entsprechenden Satz-Tokenlisten auftrennt. Orientieren Sie sich dabei an https://en.wikipedia.org/wiki/Sentence_boundary_disambiguation:\n",
    "\n",
    ">   (a) If it's a period, it ends a sentence.<br>\n",
    "    (b) If the preceding token is in the hand-compiled list of abbreviations, then it doesn't end a sentence.<br>\n",
    "    (c) If the next token is capitalized, then it ends a sentence.        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_sentences_rule_based(words):\n",
    "    sent_list = []\n",
    "    sent = []\n",
    "    for index,token in enumerate(words):\n",
    "        sent.append(token)\n",
    "        if token == 'END': #baseline\n",
    "            sent_list.append(sent)\n",
    "            sent = []\n",
    "    return sent_list  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['You',\n",
       "  'hear',\n",
       "  'that',\n",
       "  'Mr',\n",
       "  '.',\n",
       "  'Anderson',\n",
       "  '?',\n",
       "  'That',\n",
       "  'is',\n",
       "  'the',\n",
       "  'sound',\n",
       "  'of',\n",
       "  'inevitability',\n",
       "  '.',\n",
       "  'Good',\n",
       "  'bye',\n",
       "  ',',\n",
       "  'Mr',\n",
       "  '.',\n",
       "  'Anderson',\n",
       "  '!',\n",
       "  'END']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment_sentences_rule_based(test_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe 1b (Supervised Sentence Segmentation)\n",
    "\n",
    "Trainieren Sie einen Punctuation Classifier mit Hilfe der Daten zur Satzsegmentierung der Penn-Treebank. Orientieren Sie sich dabei am Vorgehen in https://www.nltk.org/book/ch06.html#sentence-segmentation:\n",
    "\n",
    "- extract features for possible sentence-boundary tokens\n",
    "- learn mapping from feature-representations to binary end-of-sentence classes (boundary yes/no)\n",
    "- training data: corpus with annotation of sentence boundaries (e.g. treebanks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 2 (Korpusannotation mit stanza)\n",
    "\n",
    "Annotieren Sie den Text in `wahlverwandschaften.txt` nach morphologischen, syntaktischen und semantischen Kategorien mit Hilfe der deutschen stanza-Modelle.\n",
    "\n",
    "Verwenden Sie dabei auch die CoNLL-Utilities von stanza für eine Transformation eines Dependency-analysierten Satzes in das CoNLL-Format, um es als NLTK-Dependency-Tree-Objekt einzulesen und zu plotten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 3 (Weiterverarbeitung Korpusannotation)\n",
    "\n",
    "Führen Sie auf dem Wahlverwandschaften-Text mit stanza ein POS-Tagging aus und verwenden Sie die Ausgabe für eine POS-Frequenzzählung und Plotting der Ergebnisse mit matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x128d662b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Code: https://www.python-graph-gallery.com/barplot/\n",
    "\n",
    "# Libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Make a random dataset:\n",
    "height = [3, 12, 5, 18, 45]\n",
    "bars = ('A', 'B', 'C', 'D', 'E')\n",
    "y_pos = np.arange(len(bars))\n",
    "\n",
    "# Create bars\n",
    "plt.bar(y_pos, height)\n",
    "\n",
    "# Create names on the x-axis\n",
    "plt.xticks(y_pos, bars)\n",
    "\n",
    "# Show graphic\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 4: Tagging mit NLTK\n",
    "\n",
    "Auch NLTK enthält Modelle für Textannotationen. Testen Sie die in den NLTK-Kapiteln 3 und 5 beschriebenen Tagger (**POS, Segmentizer, Stemmer, Lemmatizer**) für das Englische aus (wie man einen POS-Tagger mit NLTK selbst trainiert, um etwa auch auf deutschen Texten POS-Tagging mit NLTK durchzuführen, ist Thema in einer späteren Sitzung).\n",
    "\n",
    "- https://www.nltk.org/book/ch03.html\n",
    "- https://www.nltk.org/book/ch05.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
